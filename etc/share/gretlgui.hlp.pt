
# add Tests "Acrescentar variáveis ao modelo"

As variáveis seleccionadas são acrescentadas ao modelo anterior e o novo modelo é estimado. É apresentada a estatística de teste para a significância conjunta das variáveis acrescentadas, assim como o respectivo p valor. 

Menu path: Janela do modelo, /Testes/Acrescentar variáveis

Script command: <@ref="add">

# addline Graphs "Acrescentar linha a um gráfico"

Esta caixa de diálogo permite acrescentar uma linha a um gráfico, definida por uma expressão. A expressão tem que ser aceitável por gnuplot. Use <@lit="x"> para designar o valor da variável no eixo dos x. Tenha em atenção que gnuplot usa <@lit="**"> para potenciação, e que o símbolo de decimais tem que ser ".". Exemplos: 

<code>          
   10+0.35*x
   100+5.3*x-0.12*x**2
   sin(x)
   exp(sqrt(pi*x))
</code>

# adf Tests "Teste de Dickey-Fuller aumentado"

Este comando requer uma ordem desfasamento inteira; se a ordem for 0 é executado um teste Dickey–Fuller padrão (não aumentado). 

Determina estatísticas para um conjunto de testes de Dickey–Fuller sobre a variável especificada, com a hipótese nula de que a variável tem uma raiz unitária. (Mas se a opção de diferenciação tiver sido dada, a primeira diferença da variável é obtida, e a discussão abaixo deve ser interpretada como sendo referente à variável transformada.) 

Por omissão, são apresentadas três variantes do teste: uma baseada na regressão contendo uma constante, uma usando uma constante e uma tendência linear, e uma usando uma constante e uma tendência quadrática. Você pode controlar as variantes que são apresentadas ao especificar uma ou mais opções. 

Em todos os casos a variável dependente é a primeira diferença da variável especificada, <@itl="y">, e a variável independente chave é o primeiro desfasamento de <@itl="y">. O modelo é construido de modo a que o coeficiente do desfasamento de <@itl="y"> iguale 1 menos a raiz em questão. Por exemplo, o modelo com uma constante pode ser escrito como 

  <@fig="adf1">

Se a ordem de desfasamento, <@itl="k">, é maior que 0, então <@itl="k"> desfasamentos da variável dependente são incluidos no lado direito das regressões de teste, sujeitos à seguinte qualificação. Se estiver seleccionada a opção "testar para baixo a partir da ordem máxima de "lag"", a ordem de desfasamento seleccionada é considerada como sendo o desfasamento máximo e a ordem de desfasamento efectivamente usada é obtida testando para baixo, de acordo com o seguinte algoritmo: 

<indent>
1. Estimar a regressão de Dickey–Fuller com <@itl="k"> desfasamentos da variável dependente. 
</indent>

<indent>
2. O último desfasamento é significante? Se sim, executar o teste com com a ordem de desfasamento, <@itl="k">. Senão, fazer <@itl="k"> = <@itl="k"> – 1; se <@itl="k"> for igual a 0, executar o teste com a ordem de desfasamento 0, senão saltar para o passo 1. 
</indent>

No contexto do passo 2 acima, "significante" quer dizer que para o último desfasamento, a estatística-<@itl="t">, que segue uma distribuição normal, tem um <@itl="p">-value bilateral assimptótico menor ou igual a 0,10. 

Os <@itl="p">-values para os testes de Dickey–Fuller baseiam-se em MacKinnon (1996). O código relevante é incluído com a generosa permissão do autor. 

Menu path: /Variável/Teste de Dickey-Fuller aumentado

Script command: <@ref="adf">

# anova Statistics "ANOVA"

Análise de Variância: <@var="response"> é uma série que mede um efeito com interesse e <@var="treatment"> tem que ser uma variável discreta que codifica dois ou mais tipos de tratamento (ou não-tratamento). Para ANOVA de duas-vias, a variável <@var="block"> (que também deve ser discreta) codifica os valores de uma variável de controlo. 

A hipótese nula do teste <@itl="F"> é de que a resposta média é invariante com o tipo de tratamento, ou por outras palavras, que o tratamento não produz efeito. Em termos formais, o teste é apenas válido se a variância da resposta for igual para todos os tipos de tratamentos. 

Note que os resultados apresentados por este comando pertencem a um subconjunto da informação resultante do procedimento seguinte, que é facilmente implementável em gretl. Crie um conjunto de variáveis auxiliares que codifiquem todos os tipos de tratamentos, exceto um. No caso da ANOVA de duas-vias, adicionalmente, crie um conjunto de variáveis auxiliares que codifiquem todos os "blocks" (controlos), exceto um. De seguida efectue uma regressão sobre <@var="response"> com uma constante e com as variáveis auxiliares usando <@ref="ols">. No caso da ANOVA singular (ou uma-via) a tabela é produzida passando a opção <@opt="--⁠anova"> para <@lit="ols">. No caso da ANOVA de duas-vias o teste F relevante, é obtido usando o comando <@ref="omit">. Por exemplo (assumindo que <@lit="y"> é a resposta, <@lit="xt"> codifica os tratamentos, e <@lit="xb"> codifica os controlos): 

<code>          
   # uma-via
   list dxt = dummify(xt)
   ols y 0 dxt --anova
   # duas-vias
   list dxb = dummify(xb)
   ols y 0 dxt dxb
   # teste da significância conjunta de dxt
   omit dxt --quiet
</code>

Menu path: /Modelo/Outros modelos lineares/ANOVA

Script command: <@ref="anova">

# ar Estimation "Estimação autoregressiva"

Determina estimativas para os parâmetros usando o procedimento iteractivo e generalizado de Cochrane–Orcutt (ver a Secção 9.5 de Ramanathan, 2002). A iteração termina quando os erros das somas de quadrados sucessivos não difiram em mais que 0,005 porcento ou após 20 iterações. 

A "lista de desfasamentos AR" especifica a estrutura do processo de erro. Por exemplo, a entrada "1 3 4" corresponde ao processo: 

  <@fig="arlags">

Menu path: /Modelo/Série temporal/Estimação autoregressiva

Script command: <@ref="ar">

# ar1 Estimation "Estimação AR(1)"

Determina estimativas admissíveis GLS para um modelo em que se assume que o termo de erro segue um processo autoregressivo de primeira-ordem. 

O métudo por omissão é o procedimento iterativo de Cochrane–Orcutt (ver, por exemplo, a Secção 9.4 de Ramanathan, 2002). A iteração termina quando as estimativas sucessivas do coeficiente de autocorrelação não diferirem por mais de 0.001 ou após 20 iterações. 

Se tiver sido dada a opção <@lit="--hilu">, é utilizado o procedimento de pesquisa de Hildreth–Lu. Os resultados são depois aperfeiçoados usando o método Cochrane–Orcutt, exceto se tiver sido indicada a opção <@lit="--no-corc">. (Esta última opção é ignorada se não tiver sido usado <@lit="--hilu">). 

Se tiver sido dada a opção <@lit="--pwe">, é usado o estimador de Prais–Winsten. Isto involve uma iteração semelhante à de Cochrane–Orcutt; a diferença é que equanto Cochrane–Orcutt descarta a primeira observação, a de Prais–Winsten faz uso dela. Para mais detalhes ver, por exemplo, o Capítulo 13 do livro de Greene, <@itl="Econometric Analysis"> (2000). 

Menu path: /Modelo/Série temporal/Cochrane-Orcutt
Menu path: /Modelo/Série temporal/Hildreth-Lu
Menu path: /Modelo/Série temporal/Prais-Winsten

Script command: <@ref="ar1">

# arbond Estimation "Arellano-Bond"

Executa a estimação de modelos de painel dinâmico (ou seja, modelos de painel que contenham um ou mais desfasamentos da variável dependente) recorrendo ao método Método Generalizado dos Momentos (GMM-DIF) desenvolvido por <@bib="Arellano and Bond (1991);arellano-bond91">. Por favor ver <@ref="dpanel"> para uma versão mais flexível e actualizada deste comando que também usa GMM-SYS para além do GMM-DIF. 

O parâmetro <@var="p"> representa a ordem da autoregressão para a variável dependente. O parâmetro opcional <@var="q"> indica o máximo desfasamento do nível da variável dependente a ser usada como um instrumento. Se este argumento for omitido, ou de valor 0, todos os desfasamentos disponíveis são usados. 

A variável dependente deve ser dada na forma de níveis; ela será automaticamente diferenciada (pois este estimador usa diferenciação para anular os efeitos individuais). As variáveis independentes não são automaticamente diferenciadas; se você pretende usar diferenças (o que acontece em geral para variáveis quantitativas, mas não será, por exemplo, para variáveis auxiliares temporais), deve primeiro criar essas diferenças e depois especificar estas como sendo regressoras. 

O último campo (opcional) do comando serve para especificar instrumentos. Se não for dado nenhum, então é assumido que todas as variáveis independentes são estritamente exógenas. Se você especificar alguns instrumentos, então deve incluir na lista quaisquer variáveis independentes estritamente exógenas. Para regressores predeterminados, você pode usar a função <@lit="GMM"> para incluir uma gama de desfasamentos especificada no modo bloco-diagonal. Isto é ilustrado no terceiro exemplo acima. O primeiro argumento de <@lit="GMM"> é o nome da variável em questão, o segundo é o desfasamento mínimo a ser usado como instrumento, e o terceiro é o desfasamento máximo. Se o terceiro argumento for dado como 0, todos os desfasamentos disponíveis são usados. 

Por omissão são apresentados os resultados da estimação 1-fase (com erros padrão robustos). Opcionalmente, você pode escolher estimação de 2-fases. Em ambos os casos são efectuados testes de autocorrelação de ordem 1 e 2, assim como o teste de sobre-identificação de Sargan e o teste de Wald para a significância conjunta dos regressores. Note-se que este modelo de diferenciação com autocorrelação de primeira-ordem não invalida o modelo, mas que a autocorrelação de segunda-ordem não respeita as assunções estatísticas presentes. 

No caso da estimação de 2-fases, por omissão, os erros padrão são determinados usando a correcção de amostra-finita sugerida por <@bib="Windmeijer (2005);windmeijer05">. Os erros padrão assimptóticos associados ao estimador de 2-fases são em geral considerados como guias para inferência pouco fiáveis, mas se por alguma razão os pretender ver, você pode usar a opção <@lit="--asymptotic"> para desligar a correcção de Windmeijer. 

Se for dada a opção <@lit="--time-dummies">, são acrescentadas variáveis auxiliares temporais aos regressores especificados. Para evitar colinearidade perfeita com a constante, o número de auxiliares é uma unidade a menos que o número máximo de períodos usados na estimação. As auxiliares são introduzidas por níveis; se você deseja usar auxiliares de tempo na forma de primeiras-diferenças, você terá que definir e acrescentar essas variáveis manualmente. 

Menu path: /Modelo/Painel/Arellano-Bond

Script command: <@ref="arbond">

# arch Estimation "Modelo ARCH"

This command is retained at present for backward compatibility, but you are better off using the maximum likelihood estimator offered by the <@ref="garch"> command; for a plain ARCH model, set the first GARCH parameter to 0. 

Estima a especificação do modelo fornecido aceitando em ARCH (Heterosquedicidade Condicional Autoregressiva). O modelo é primeiramente estimado em OLS, e depois é efectuada uma regressão auxiliar, na qual, o quadrado dos resíduos da primeira fase é regredido com os seus próprios valores desfasados. A fase final é uma estimação por mínimos quadrados com pesos, usando como pesos os recíprocos das variâncias de erro ajustadas da regressão auxiliar. (Se a variância predita de de alguma observação na regressão auxiliar for não positiva, então será usada o correspondente resíduo quadrado). 

Os valores <@lit="alpha"> mostrados abaixo dos coeficientes são os parâmetros estimados do processo ARCH da regressão auxiliar. 

Ver também <@ref="garch"> e <@ref="modtest"> (a opção <@opt="--⁠arch">). 

Menu path: /Modelo/Série temporal/ARCH

Script command: <@ref="arch">

# arima Estimation "Modelo ARMA"

Estima um modelo ARMA, com ou sem regressores exógenos. Se a ordem de diferenciação for superior a zero o modelo passa a ser ARIMA. Se os dados têm uma frequência superior a 1, é apresentada a opção de inclusão de uma componente sazonal. 

Se você pretender apenas incluir no modelo desfasamentos específicos AR ou MA (e não todos os desfasamentos até uma certa ordem) ative a caixa de seleção à direita do seletor de números e escreva no campo de texto uma lista de desfasamentos, separados por espaços. Alternativamente, se existir uma matriz contendo o conjunto de desfasamentos pretendidos, você pode introduzir o seu nome no campo de texto. 

O normal é usar a funcionalidade "nativa" gretl ARMA, com estimação de Máxima Verosimilhança (ML) exata (usando o filtro de Kalman). Outras opções são: código nativo, ML condicional; X-12-ARIMA, ML exata; e X-12-ARIMA, ML condicional. (As últimas duas opções estão disponíveis apenas se o programa X-12-ARIMA estiver instalado.) Para detalhes sobre estas opções, veja por favor <@pdf="the Gretl User's Guide">. 

O valor AIC retornado em ligação com os modelos ARIMA é calculado conforme a definição usada no programa X-12-ARIMA, nomeadamente 

  <@fig="aic">

onde <@fig="ell"> é o logaritmo da verosimilhança e <@itl="k"> é o número total de parâmetros estimados. Note-se que o programa X-12-ARIMA não produz critérios de informação tal como o AIC quando a estimação é por ML condicional. 

As raízes AR e MA apresentadas em ligação com a estimação ARMA são baseadas na seguinte representação de um processo ARMA(p,q): 

<mono>          
   (1 - a_1*L - a_2*L^2 - ... - a_p*L^p)Y =
          c + (1 + b_1*L + b_2*L^2 + ... + b_q*L^q) e_t
</mono>

As raízes AR são portanto as soluções de 

<mono>          
         1 - a_1*z - a_2*z^2 - ... - a_p*L^p = 0
</mono>

e a estabilidade requer que estas raízes estejam fora do círculo unitário. 

A imagem da "frequência" apresentada em ligação com as raízes AR e MA é o valor λ que resolve <@itl="z"> = <@itl="r"> * exp(i*2*π*λ) onde <@itl="z"> é a raiz em questão e <@itl="r"> o seu módulo. 

Menu path: /Modelo/Série temporal/ARIMA
Other access: Menu de contexto da janela principal (selecção singular)

Script command: <@ref="arima">

# bfgs-config Estimation "Opções do maximizador BFGS"

Esta janela de diálogo permite-lhe controlar alguns aspetos de operação do maximizador BFGS. Se o maximizador falhar em convergir, em alguns casos, pode ajudar aumentar o número máximo de iterações e, ou, aumentar a tolerância de convergência (ser mais permissivo). No entanto, se tiver sido usada uma tolerância muito alta, esses resultados devem ser encarados com desconfiança pois é possível que o modelo que se está a estimar não tenha sido bem especificado. 

Na maior parte das aplicações nós recomendamos o uso do maximizador BFGS normal, mas para algums problemas a variante do algoritmo com "memória limitada", L-BFGS-B, pode produzir uma convergência mais rápida. Quando se seleciona L-BFGS-B, tem-se a opção de definir o número de correções usadas na matriz de memória limitada (entre 3 e 20, com um valor por omissão de 8). 

# bootstrap Tests "Opções do Bootstrap"

Nesta janela de diálogo você pode escolher: 

<indent>
• A variável ou coeficiente a examinar. (Com este método, você apenas pode testar um coeficiente de cada vez.) 
</indent>

<indent>
• O tipo de análise a efectuar. O intervalo de confiança (95 porcento) omissão baseia-se diretamente nos quantis das estimativas dos coeficientes bootstrap. A versão "studentized" segue o texto de Davidson e MacKinnon <@itl="Economic Theory and Methods"> (ETM), Capítulo 5: em cada replicação bootstrap uma razão-<@itl="t"> é construída como (a) a diferença entre as estimativas do coeficiente corrente e da linha-base, dividida pela (b) estimativa do erro padrão da linha-base. Então o intervalo de confiança é construído segundo os quantis desta razão-<@itl="t">, tal como explicado no ETM. A opção valor-P baseia-se na distribuição da razão-<@itl="t"> do bootstrap: é a proporção de replicações onde o valor absoluto desta estatística excede o valor absoluto da razão-<@itl="t"> da linha-base. 
</indent>

<indent>
• Resíduos reamostrados versus erros normalizados simulados. No primeiro caso os resíduos originais (re-escalados como sugerido em ETM) são reamostrados com substituição. No segundo caso são gerados valores normalizados pseudo-aleatórios a partir da variância original dos resíduos. 
</indent>

<indent>
• O número de replicações a realizar. Note que quando você está a construir um intervalo de confiança a 95 porcento, é desejável que 0.05(<@itl="B"> + 1)/2 resulte num valor inteiro (onde <@itl="B"> é o número de replicações). Portanto o gretl pode ajustar o número de replicações de modo a garantir isso. 
</indent>

<indent>
• Se se produz ou não um gráfico da distribuição bootstrap. Esta opção usa o mecanismo de estimação de densidade de núcleo do gretl. 
</indent>

# boxplot Graphs "Gráficos caixa-com-bigodes"

Estes gráficos apresentam a distribuição de uma variável. A caixa central contém os 50 porcento dos dados centrais, i.e. está limitada pelos primeiro e terceiro quartis. Os "bigodes" estendem-se até aos valores mínimo e máximo. É desenhada uma linha que corta a caixa na mediana. Um símbolo "+" indica a média. Se tiver sido selecionada a opção de mostrar o intervalo de confiança para a média, ele é obtido pelo método 'bootstrap' e apresentado na forma de linhas a tracejado acima e abaixo da mediana. 

A opção <@opt="--⁠factorized"> permite examinar a distribuição de uma variável condicionada pelo valor de um fator discreto. Por exemplo, se um conjunto de dados contém salários e uma variável auxiliar para o sexo, você pode selecionar a variável salário como alvo e o sexo como fator, para visualizar lado a lado gráficos caixas-com-bigodes dos salários de homens e mulheres. 

Menu path: /Ver/Gráfico das variáveis/Caixa com bigodes

Script command: <@ref="boxplot">

# bwfilter Transformations "Filtro de Butterworth"

O filtro de Butterworth é uma aproximação a um filtro ideal de onda-quadrada que deixa passar frequências de uma certa gama com potência máxima enquanto bloqueia todas as outras. 

Em princípio, valores altos do parâmetro de ordem, <@itl="n">, produzem uma melhor aproximação ao filtro ideal, mas tem uma possível penalidade de instabilidade numérica. O valor de "corte" define a banda de passagem e a banda de bloqueio. Ele é expresso em graus, e tem que ser maior que 0 e menor que 180° (ou π radianos, correspondendo à maior frequência nos dados). Valores menores do corte produzem uma tendência mais suave (alisamento). 

Observar o periodograma da série em estudo é uma análise preliminar que é útil quando se pretende aplicar este filtro. Para mais detalhes ver <@pdf="the Gretl User's Guide">. 

Menu path: /Variável/Filtro/Butterworth

# chow Tests "Teste de Chow"

Este comando requer um número de uma observação (ou uma data, em dados cronológicos). 

Tem que se seguir a uma regressão de Mínimos Quadrados (OLS). Se um número de observação ou uma data tiver sido dado, produz um teste sobre a hipótese nula de não haver quebra estrutural no ponto de separação indicado. O procedimento cria uma variável auxiliar que é igual a 1 a partir do ponto especificado por <@var="observação"> até ao final da amostra, caso contrário é 0, e cria também termos de interação entre esta variável auxiliar e as variáveis regressoras originais. Se tiver sido dada uma variável auxiliar, será testada a hipótese nula de homogeneidade estrutural no que diz respeito a essa variável auxiliar. Mais uma vez, os termos de interação são acrescentados. Em quaisquer dos casos, é executada uma regressão aumentada que inclui estes termos e é calculada a estatística <@itl="F">, considerando a regressão aumentada como não restringida e a original como restringida. Mas se o modelo original usou um estimador robusto para a matriz de covariância, a estatística de teste é um valor de qui-quadrado de Wald baseada num estimador robusto da matriz de covariância da regressão aumentada. 

Menu path: Janela do modelo, /Testes/Teste de Chow

Script command: <@ref="chow">

# cluster Estimation "Estimação de variância robusta"

Se você selecionar a segunda opção então tem que fornecer o nome de uma variável agrupadora. Esta variável deve ter pelo menos dois valores distintos mas geralmente deve ter substancialmente menos valores distintos do que há observações no intervalo da amostra. 

O estimador de variância "agrupação-robusta" divide a amostra num número de subconjuntos ou agrupamentos de acordo com o valor tomado na variável selecionada. Em vez da assunção clássica de que o erro é independente e identicamente distribuído, este estimador permite que a variância do erro seja diferente por agrupamento e também permite algum grau de dependência do erro dentro de cada agrupamento. 

# coeffsum Tests "Soma de coeficientes"

Este comando requer uma lista de variáveis, escolhidas a partir do conjunto de variáveis independentes de um dado modelo. 

Calcula a soma dos coeficientes nas variáveis indicadas na lista. Apresenta esta soma juntamente com o seu erro padrão e o p-value para a hipótese nula de que a soma é zero. 

Note-se a diferença entre este teste e <@ref="omit">, que testa a hipótese nula de que os coeficientes num conjunto especificado de variáveis independentes são <@itl="todos"> iguais a zero. 

Menu path: Janela do modelo, /Testes/Soma de coeficientes

Script command: <@ref="coeffsum">

# coint Tests "Teste de cointegração Engle-Granger"

O teste de cointegração Engle–Granger. O procedimento por omissão é: (1) efetuar testes de Dickey–Fuller (DF) segundo a hipótese nula de que cada variável listada tem uma raiz unitária; (2) estima a regressão de cointegração; e (3) executar um teste DF sobre os resíduos da regressão de cointegração. No entanto, se estiver selecionada a opção "ignorar os testes iniciais DF" os testes no passo (1) são omitidos. 

Se a ordem de desfasamento <@itl="k">, é maior que zero, então são incluídos <@itl="k"> desfasamentos no lado direito de cada regressão de teste, excepto se estiver selecionada a opção "Testar para baixo a partir do desfasamento de maior ordem": nesse caso ela é encarada como sendo o máximo desfasamento e a efetivamente usada em cada caso é obtida testando para baixo. Ver o comando <@ref="adf"> para detalhes sobre este procedimento. 

Por omissão, as regressões de cointegração incluem uma constante. Se você deseja suprimir a constante, ou juntar uma tendência linear ou quadrática, selecione a opção apropriada a partir do conjunto de botões de rádio disponíveis na janela do teste de cointegração. 

Os <@itl="P-">values para este teste são baseados em MacKinnon (1996). O código relevante é incluído com a generosa permissão do autor. 

Menu path: /Modelo/Série temporal/Testes de Cointegração/Engle-Granger

Script command: <@ref="coint">

# coint2 Tests "Teste de cointegração de Johansen"

Executa o teste de Johansen para a cointegração entre as variáveis em <@var="listaY"> para a dada ordem de desfasamento. Para detalhes sobre este teste ver <@pdf="the Gretl User's Guide"> ou <@bib="Hamilton (1994);hamilton94">, Capítulo 20. Os valores p são calculados usando a aproximação gama de Doornik <@bib="(Doornik, 1998);doornik98">. São mostrados dois conjuntos de valores p para o teste traço, valores assintóticos imediatos e valores ajustados para o tamanho da amostra. Por omissão, o acessor <@lit="$pvalue"> obtém a variante ajustada, mas se opção <@opt="--⁠asy"> for dada, pode ser usado para registar os valores assintóticos. 

Efetua o teste de Johansen para a cointegração entre as variáveis listadas para a ordem de desfasamento selecionada. Para detalhes sobre este teste ver, por exemplo, o livro de Hamilton, <@itl="Time Series Analysis"> (1994), Capítulo 20. Os valores p são calculados usando a aproximação gama de Doornik. 

A inclusão de termos determinísticos no modelo é controlada por intermédio dos botões das opções. Por omissão, se não tiver sido indicada nenhuma opção, será incluída uma "constante não restringida", o que permite a presença de um interceptor não-nulo nas relações cointegrantes assim como uma tendência nos níveis das variáveis endógenas. Na literatura derivada do trabalho de Johansen (ver por exemplo o livro dele de 1995) isto é frequentemente referido como sendo o "caso 3". Os outros quatro botões de opções produzem respectivamente os casos 1, 2, 4, e 5. O significado destes casos e os critérios para selecionar um caso estão explicados no <@pdf="the Gretl User's Guide">. 

Você pode controlar as variáveis exógenas adicionando-as à lista na caixa inferior. Por omissão estas entram no modelo na forma não restringida (indicada por um <@lit="U"> junto do nome da variável). Se você pretender que uma certa variável seja restringida ao espaço de cointegração, clique com botão direito sobre ela e selecione "Restringida" do menu de contexto. O símbolo junto à variável mudará para R. 

Se os dados são trimestrais ou mensais, fica disponível a seleção de uma opção que lhe permite incluir um conjunto de variáveis auxiliares sazonais centradas. Em todos os casos, a opção ("Mostrar detalhes") permite apresentar as regressões auxiliares que formam o ponto de partida do procedimento da estimação de máxima verosimilhança de Johansen. 

A seguinte tabela serve como um guia à interpretação dos resultados apresentados pelo teste, num caso de 3-variáveis. <@lit="H0"> significa a hipótese nula, <@lit="H1"> a hipótese alternativa, e <@lit="c"> o número de relações cointegrantes. 

<mono>          
         Ordem    Teste Traço        Teste Lmax
                  H0     H1          H0     H1 
         ---------------------------------------
          0      c = 0  c = 3       c = 0  c = 1
          1      c = 1  c = 3       c = 1  c = 2
          2      c = 2  c = 3       c = 2  c = 3
         ---------------------------------------
</mono>

Ver também o comando <@ref="vecm">. 

Menu path: /Modelo/Série temporal/Testes de Cointegração/Johansen

Script command: <@ref="coint2">

# compact Dataset "Compactar dados"

Quando você acrescenta a um conjunto de dados uma série temporal cuja frequência é superior, é necessário "compactar" a nova série. Por exemplo, uma série mensal terá que ser compactada para caber num conjunto de dados trimestral. 

Adicionalmente, por vezes você pode querer compactar um conjunto de dados para uma frequência mais baixa (eventualmente, antes de acrescentar uma variável de baixa-frequência ao conjunto de dados). 

Gretl oferece quatro opções de compactação: 

<indent>
• Média: O valor escrito no conjunto de dados será a média aritmética dos valores relevantes da série temporal. Por exemplo, o valor escrito para o primeiro trimestre de 1990 será a média dos valores de janeiro, fevereiro e março de 1990. 
</indent>

<indent>
• Soma: O valor escrito no conjunto de dados será a soma dos valores de alta-frequência relevantes. Por exemplo, o valor escrito para o primeiro trimestre será a soma dos valores de janeiro, fevereiro e março. 
</indent>

<indent>
• Valores fim-do-período: O valor escrito no conjunto de dados será o último valor relevante dos dados de alta-frequência. Por exemplo, o valor escrito para o primeiro trimestre de 1990 será o valor de março de 1990. 
</indent>

<indent>
• Valores início-do-período: O valor escrito no conjunto de dados será o primeiro valor relevante dos dados de alta-frequência. Por exemplo, o valor escrito para o primeiro trimestre de 1990 será o valor de janeiro de 1990. 
</indent>

Caso esteja a compactar a totalidade de um conjunto de dados, a escolha que você fizer neste diálogo, define o método por omissão. Mas se definiu um método de compactação individualmente para uma variável (no menu, "Variável/Editar características"), esse método será usado e não o por omissão. Se o método de compactação já estiver definido para todas as variáveis, não será apresentada a escolha do método. 

Menu path: /Dados/Compactar Dados

# controlled Graphs "Gráfico X-Y com controlo"

Este comando requer a seleção de três variáveis, uma para o eixo dos X, uma para o eixo dos Y, e outra para a que você deseja controlar (chamemos-lhe Z). O gráfico apresenta Y ajustado contra X ajustado, onde a versão ajustada da variável são os resíduos de uma regressão por Mínimos Quadrados (OLS) sobre Z. 

Exemplo: Você tem uma amostra com dados de salários, experiência e nível de educação de um conjunto de pessoas. Você pretende ver um gráfico dos salários contra o nível de educação controlando pela experiência. Nesse caso você seleciona salários para o eixo dos Y, nível de educação para o eixo dos X, e experiência como controlo. O gráfico apresenta os salários contra a educação, com ambas as variáveis "purgadas" do efeito experiência. 

# corrgm Statistics "Correlograma"

Apresenta os valores da função de autocorrelação para a <@var="série">, que pode ser especificada por nome ou por número. Os valores são definidos como ρ(<@itl="u"><@sub="t">, <@itl="u"><@sub="t-s">) onde <@itl="u"><@sub="t"> é a <@itl="t">–ésima observação da variável <@itl="u"> e <@itl="s"> é o número de desfasamentos. 

Também são apresentadas as autocorrelações parciais (obtidas segundo o algoritmo de Durbin–Levinson): estas constituem a rede dos efeitos dos desfasamentos intervenientes. Adicionalmente, é apresentada a estatística de teste <@itl="Q"> de Ljung–Box. Esta pode ser usada para testar a hipótese nula de que a série é "ruído branco": terá uma distribuição qui-quadrado assimptótico com os graus de liberdade iguais ao número de desfasamentos usados. 

Se o valor <@var="ordem"> for especificado o comprimento do correlograma fica limitado a esse máximo número de desfasamentos, senão o comprimento é determinado automaticamente, como uma função da frequência dos dados e do número de observações. 

Por omissão é apresentado um gráfico do correlograma: um gráfico gnuplot em modo interativo ou um gráfico ASCII em modo de lote de comandos. Isto pode ser ajustado por via da opção <@opt="--⁠plot">. Os parâmetros válidos para esta opção são <@lit="none"> (para suprimir o gráfico); <@lit="ascii"> (para produzir um gráfico de texto mesmo em modo interativo); <@lit="display"> (para produzir um gráfico gnuplot mesmo em modo de lote de comandos); ou o nome de um ficheiro. O efeito de se fornecer um nome de ficheiro é como descrito para a opção <@opt="--⁠output"> do comando <@ref="gnuplot">. 

Depois de completar com sucesso, os acessores <@lit="$test"> e <@lit="$pvalue"> contêm os respectivos valores do teste de Ljung–Box para a maior ordem apresentada. Note que se você apenas quiser determinar a estatística <@itl="Q">, provavelmente você quererá usar a função <@xrf="ljungbox">. 

Menu path: /Variável/Correlograma
Other access: Menu de contexto da janela principal (selecção singular)

Script command: <@ref="corrgm">

# count-model Estimation "Modelos para a contagem de dados"

A variável dependente é tida como representando a contagem de ocorrência de eventos de alguma natureza, e tem que ter apenas valores inteiros não-negativos. Por omissão, usa-se a distribuição de Poisson, mas o seletor de menu permite escolher a distribuição Binomial Negativa. (A variante 'NegBin 2' é normalmente usada em econometria, mas também está disponível a menos usada 'NegBin 1'). 

Opcionalmente, você pode acrescentar na especificação uma variável "offset". Esta é uma variável de escala, cujo logarítmo é acrescentado à função de regressão linear (implicitamente com um coeficiente de 1.0). Isto faz sentido quando você espera que o número de ocorrências do evento em questão seja proporcional a algum factor conhecido (TODO: other things equal). Por exemplo, o número de acidentes de trânsito pode ser assumido como proporcional ao volume de trânsito (TODO: other things equal), e nesse caso o volume de trânsito pode ser especificado como um "offset" num modelo de taxa de acidentes. A variável 'offset' tem que ser estritamente positiva. 

Por omissão os erros padrão são calculados usando uma aproximação numérica da Hessiana por convergência. Mas se estiver seleccionada a caixa dos "Erros padrão robustos" então serão calculados os erros padrão QML, usando uma "sandwich" da inversa da Hessiana e do produto externo do gradiente. 

# curve Graphs "Desenha uma curva"

Esta caixa de diálogo permite-lhe criar um gráfico gnuplot a partir de uma expressão. Esta tem que ser uma expressão válida para o gnuplot. Use <@lit="x"> como o identificador dos valores da variável no eixo dos X. Tenha em atenção que o gnuplot usa <@lit="**"> para potenciação, e que o caratere dos decimais tem que ser o ".". Exemplos: 

<code>          
   10+0.35*x
   100+5.3*x-0.12*x**2
   sin(x)
   exp(sqrt(pi*x))
</code>

Para acrescentar uma linha adicional a um gráfico criado por este processo, clicar no gráfico e selecionar "Editar", selecionar o separador "Linhas" na janela de edição do gráfico, e use o botão "Acrescentar linha...". 

# cusum Tests "Teste CUSUM"

Tem que se seguir à estimação de um modelo por via de OLS. Executa o teste CUSUM —ou se for dada a opção <@lit="--squares">, o teste CUSUMSQ —para a estabilidade dos parâmetros. É obtida uma série temporal de erros de predição um passo-à-frente, pela execução de séries de regressões: a primeira regressão usa as primeiras <@itl="k"> observações e é usada para gerar a predição da variável dependente na observação <@itl="k"> + 1; a segunda usa a primeira predição para a observação <@itl="k"> + 2, e por aí a diante (onde <@itl="k"> é o número de parâmetros no modelo original). 

A soma acumulada dos erros de predição escalados, ou os quadrados desses erros, é mostrada e apresentada em gráfico. A hipótese nula para a estabilidade dos parâmetros é rejeitada ao nível de cinco porcento, se a soma acumulada se desviar do intervalo de confiança de 95 porcento. 

No caso do teste CUSUM, é também apresentada a estatística de teste <@itl="t"> de Harvey–Collier, para a hipótese nula da estabilidade dos parâmetros. Ver o livro <@itl="Econometric Analysis"> de Greene para mais detalhes. Para o teste CUSUMSQ, o intervalo de confiança a 95 porcento é calculado de acordo com o algoritmo apresentado por <@bib="Edgerton e Wells (1994);edgerton94">. 

Menu path: Janela do modelo, /Testes/Teste CUSUM(SQ)

Script command: <@ref="cusum">

# datasort Dataset "Ordenar dados"

A variável selecionada é usada como a chave de ordenação para todo o conjunto de dados. As observações em todas as variáveis são re-ordenadas por ordem crescente dos valores da variável chave, ou por valores decrescentes caso tenha selecionado a opção "Descrescente". 

# density Statistics "Estimação de densidade de núcleo"

O procedimento de estimação de densidade de núcleo cria um conjunto de pontos de referência igualmente espaçados, ao longo de um intervalo adequado em relação ao intervalo dos dados, e atribui uma densidade a cada ponto de referência baseado nas observações reais na vizinhança. 

A função utilizada para determinar a densidade estimada em cada ponto de referência, <@itl="x">, é 

  <@fig="kernel1">

onde <@itl="n"> é o número de pontos de dados, <@itl="h"> é o parâmetro de "largura de banda", e <@itl="k">() é a função núcleo. Quanto maior for o valor da largura de banda, mais alisada será a densidade estimada. 

Você pode escolher o núcleo Gaussiano (a densidade normal padrão) ou núcleo de Epanechnikov. Por omissão, a largura de banda é a sugerida pela regra de ouro de <@bib="Silverman (1986);silverman96">, nomeadamente 

  <@fig="kernel2">

onde <@itl="s"> designa o desvio padrão dos dados e IQR é a amplitude inter-quartil. Você pode alargar ou encolher a largura de banda por meio do "fator de ajustamento de largura de banda": a largura de banda final é a obtida pela multiplicação do valor de Silverman pelo fator de ajustamento. 

Para uma boa introdução sobre a estimação de densidade de núcleo ver o Capítulo 15 do livro de Davidson e MacKinnon, <@itl="Econometric Theory and Methods">. 

# dfgls Tests "O teste ADF-GLS"

O teste ADF-GLS é uma variante do teste de Dickey–Fuller para uma raiz unitária, para o caso onde se assume que a variável a ser testada tem uma média não-nula ou manifesta uma tendência linear. A diferença está no processo de anulação da média ou da têndencia da variável ser feito com o procedimento GLS sugerido por Elliott, Rothenberg e Stock (1996). Isto resulta num teste mais potente do que o da abordagem padrão de Dickey–Fuller. 

Ver também o comando <@ref="adf"> e a opção <@opt="--⁠gls">. 

Menu path: /Variável/Testes de raiz unitária/Teste ADF-GLS

# dialog Estimation "Janela de diálogo do modelo"

Para selecionar a variável dependente, escolher uma variável na lista à esquerda e clicar no botão "Escolher" que aponta para o espaço da variável dependente. Se você activar a caixa de seleção "Definir por omissão", a variável selecionada ficará pré-selecionada nas próximas vezes que abrir uma janela de diálogo do modelo. Atalho: Fazer clique-duplo numa variável à esquerda para a selecionar como variável dependente e também defini-la como por omissão. 

Para selecionar as variáveis independentes, escolhê-las na lista à esquerda e clicar no botão "Acrescentar" (ou clicar o botão direito do rato). Você pode escolher um grupo não-contínuo de variáveis clicando nelas mantendo a tecla <@lit="Ctrl"> pressionada. 

# dpanel Estimation "Modelos de painel dinâmico"

Efectua a estimação de modelos de dados de painel dinâmico (ou seja, modelos de painel que incluem um ou mais desfasamentos da variável dependente) usando os métodos GMM-DIF ou GMM-SYS. 

A variável dependente e os regressores devem ser dados na forma de níveis; eles serão automaticamente diferenciados (pois este estimador usa diferenciação para anular os efeitos individuais). 

No que diz respeito à manipulação de instrumentos, veja a documentação da versão de sequência de comandos deste comando. Presentemente você não pode especificar no interface gráfico (GUI) instrumentos explicítamente: todas as variáveis independentes são consideras como sendo estritamente exógenas. 

Por omissão são apresentados os resultados da estimação a um passo (juntamente com erros padrão robustos). Opcionalmente você pode selecionar a estimação a dois passos. Em ambos os casos são determinados os testes de autocorrelação de ordem 1 e 2, assim como o teste de Sargan para a sobre-identificação e o teste de Wald para a significância conjunta dos regressores. Note que neste modelo diferenciado a autocorrelação de primeira ordem não é um risco para a validade do modelo, mas a autocorrelação de segunda ordem viola as assunções estatísticas presentes. 

Para mais detalhes e exemplos, ver o <@pdf="the Gretl User's Guide">. 

Menu path: /Modelo/Painel/Modelo de painel dinâmico

Script command: <@ref="dpanel">

# expand Dataset "Expandir dados"

Se você pretender acrescentar a um conjunto de dados uma série que tenha uma frequência inferior, será necessário "expandir" a nova série. Por examplo, uma série trimestral terá que ser expandida para caber dentro de um conjunto de dados mensal. Adicionalmente, você pode desejar expandir a totalidade de um conjunto de dados para uma frequência superior (eventualmente, antes de acrescentar uma variável de alta-frequência ao conjunto de dados). 

Expandir dados deve ser considerada uma opção para "especialistas"; você tem que saber o que está a fazer. Ao combinar séries de diferentes frequências dentro de um conjunto de dados, você deve preferencialmente compactar os dados de alta-frequência em vez de expandir as séries baixa-frequência. 

Dito isto, gretl oferece duas opções: valores de alta-frequência podem ser interpolados usando o método de Chow e Lin (1971), ou os valores de seéries de baixa-frequência podem ser repetidos o número de vezes necessárias. 

O método de Chow-Lin baseia-se em regressão, usando uma constante e uma tendência quadrática e assumindo um processo autoregressivo de primeira ordem para as perturbações. Este procedimento usa quatro graus de liberdade. Quanto à repetição de valores, suponha que temos uma série trimestral com o valor 35,5 em 1990:1, o primeiro trimestre de 1990. Ao expandir para mensal, o valor 35,5 será atribuído às observações de janeiro, fevereiro e março de 1990. A variável expandida será assim inútil para análises apuradas de séries temporais, excetuando o caso especial onde você sabe que a variável em questão se mantém de fato constante ao longo dos subperíodos. 

# export Dataset "Exportar dados"

Voce pode exportar dados no formato Valores Separados por Vírgulas (CSV): esse tipo de dados pode ser aberto em folhas-de-cálculo e em muitos outros programas. Se você escolher esta opção poderá selecionar outras opções sobre o formato específico do ficheiro CSV. 

Você também tem a possibilidade de exportar dados na forma "nativa" de ficheiro-de-dados gretl, ou (se os dados forem adequandos) exportar para uma base-de-dados gretl. Ver <@url="gretl.sourceforge.net/gretl_data.html"> para uma listagem de bases-de-dados gretl. 

Você também pode exportar os dados num formato adequado para usar nos seguintes programas: 

<indent>
• GNU R (<@url="www.r-project.org">) 
</indent>

<indent>
• GNU octave (<@url="www.gnu.org/software/octave">) 
</indent>

<indent>
• JMulTi (<@url="www.jmulti.de">) 
</indent>

<indent>
• PcGive (<@url="www.pcgive.com">) 
</indent>

Se você quiser exportar dados copiando para a memória em vez de escrever para um ficheiro em disco, selecione a série que você quer copiar na janela principal, clique com o botão direito e selecione "Copiar para a memória de edição". (Neste contexto apenas é suportado o formato CSV.) 

# factorized Graphs "Gráfico com separação fatorizada"

Este comando requer a seleção de três variáveis sendo a última delas uma variável auxiliar (valores 1 ou 0). A variável Y é representada contra a variável X, com os pontos coloridos diferentemente de acordo com o valor da terceira. 

Examplo: Você tem dados de salários e níveis de educação obtidos numa amostra de pessoas; você tem também uma variável auxiliar com valores 1 para homens e 0 para mulheres (tal como nos dados <@lit="data7-2"> de Ramanathan). Um "gráfico com separação fatorizada" de <@lit="WAGE"> contra <@lit="EDUC"> usando a variável auxiliar <@lit="GENDER"> como fator mostrará os pontos de dados para homens numa cor e os das mulheres noutra (com uma legenda para as identificar). 

# fcast Prediction "Gerar predições"

Tem que se seguir a um comando de estimação. A predições são geradas para o intervalo de observações especificado. Dependendo da natureza do modelo, também podem ser gerados erros padrão (ver abaixo). 

A escolha entre predições estáticas ou dinâmicas aplica-se apenas no caso de modelos dinâmicos, com processamento autoregressivo de erros ou que incluam um ou mais valores desfasados da variável dependente como regressores. As predições estáticas são um passo à frente, baseadas em valores concretizados no período anterior, enquanto que as predições dinâmicas usam a regra de encadeamento de predição. Por exemplo, se uma predição de <@itl="y"> em 2008 requer como entrada um valor de <@itl="y"> em 2007, uma predição estática é impossível sem dados reais para 2007. Uma predição dinâmica para 2008 é possível se uma predição anterior poder ser substítuida no <@itl="y"> em 2007. 

Por omissão o normal é produzir uma predição estática para alguma parte do intervalo de predição que abrange o intervalo da amostra onde o modelo foi estimado, e uma predição dinâmica (se relevante) para fora-da-amostra. A opção <@lit="dynamic"> chama uma predição dinâmica a partir da date mais cedo possível, e a opção <@opt="--⁠static"> chama uma predição estática até para fora-da-amosta. 

<code>          
   fcast --plot=fc.pdf
</code>

produzirá um gráfico em formato PDF. Serão respeitados caminhos completos para ficheiros, senão os ficheiros são escritos na diretoria de trabalho do gretl. 

A natureza dos erros padrão de predição (se disponíveis) depende da natureza do modelo e da predição. Para modelos lineares estáticos os erros padrão são determinados usando o método traçado por <@bib="Davidson e MacKinnon (2004);davidson-mackinnon04">; eles incorporam tanto a incerteza devida aos processos de erro como a incerteza dos parâmetros ( resumidos na matriz de covariância das estimativas dos parâmetros). Para modelos dinâmicos, os erros padrão de predição são apenas calculados no caso de uma predição dinâmica, e eles não incorporam incerteza de parâmetros. Para modelos não-lineares, os erros padrão de predição não estão disponíveis atualmente. 

Menu path: Janela de Modelo, /Análise/Predições...

Script command: <@ref="fcast">

# fractint Statistics "Integração fracional"

Testa a integração fracional sobre a série especificada ("memória longa"). A hipótese nula é de que a ordem de integração da série é zero. Por omissão é usado o estimador local de Whittle <@bib="(Robinson, 1995);robinson95"> mas se tiver sido dada a opção <@opt="--⁠gph"> será usado o teste GPH <@bib="(Geweke e Porter-Hudak, 1983);GPH83">. Se a opção <@opt="--⁠all"> for dada então serão mostrados os resultados dos dois testes. 

Para mais detalhes sobre este tipo de testes, ver <@bib="Phillips e Shimotsu (2004);phillips04">. 

Se não tiver sido dado o argumento opcional <@var="ordem">, a ordem para os teste(s) é automaticamente definida como sendo o menor de <@itl="T">/2 e <@itl="T"><@sup="0.6">. 

Os resultados podem ser obtidos usando os acessores <@lit="$test"> e <@lit="$pvalue">. Estes valores baseiam-se no estimador local de Whittle exceto quando dada a opção <@opt="--⁠gph">. 

Menu path: /Variável/Testes de raiz unitária/Integração fracional

Script command: <@ref="fractint">

# freq Statistics "Distribução de frequência"

Na janela de diálogo do gráfico de frequência você pode controlar as caraterísticas do gráfico em duas maneiras diferentes. 

Primeiro, você pode escolher o número de classes. Neste caso a largura e localização das classes são calculadas automaticamente. 

Em alternativa, você pode especificar o limite inferior da classe mais à esquerda, e a largura das classes. Neste caso o número de classes é calculado automaticamente. 

Se você desejar alinhar as classes para números redondos, esta é uma maneira possível: comece por especificar o número de classes, e observe o gráfico produzido. Se não fôr do seu agrado, anote a modificação necessária (por exemplo, fazer a classe mais à esquerda iniciar em 100 e impor uma largura de classe de 200). Então efectue uma segunda passagem onde você especifica o limite da classe mais à esquerda e a largura das classes. 

Este diálogo também permite selecionar a apresentação da curva da distribuição teórica dos dados: a normal ou a gama. Se a opção normal for selecionada será determinado o teste para normalidade de Doornik–Hansen. Se a opção gama é selecionada, gretl calcula o teste não paramétrico de Locke para a hipótese nula de que variável segue uma distribuição gama. Note que a parametrização da distribuição gama utilizadada em gretl é (forma, escala). 

Menu path: /Variável/Distribuição de frequência

Script command: <@ref="freq">

# garch Estimation "Modelo GARCH"

Estima um modelo GARCH (GARCH = Autoregressivo Generalizado de Heterocedastidade Condicional, "Generalized Autoregressive Conditional Heteroskedasticity"), que pode ser um modelo univariado, ou multivariado se selecionadas variáveis independentes, incluindo as variáveis exógenas. Em baixo é mostrada a equação de variância condicional: 

  <@fig="garch_h">

Portanto, o parâmetro <@var="p"> representa a ordem Generalizada (ou "AR"), enquanto <@var="q"> representa a ordem normal ARCH (ou "MA"). Se <@var="p"> for não-nulo, <@var="q"> tem também que ser não-nulo senão o modelo fica não-identificado. No entanto, você pode estimar um modelo ARCH normal ao definir <@var="q"> para um valor positivo e <@var="p"> para zero. A soma de <@var="p"> e <@var="q"> não pode ser maior que 5. 

Por omissão a estimação de modelos GARCH é feita usando código nativo gretl, mas você também tem a possibilidade de usar o algoritmo de <@bib="Fiorentini, Calzolari e Panattoni (1996);fiorentini96">. O primeiro usa o maximizador BFGS enquanto o segundo usa a matriz de informação para maximizar a verosimilhança, com aperfeiçoamento por via da Hessiana. 

Para este comando estão disponíveis diferentes estimadores da matriz de covariância. Por omissão, usa-se a Hessiana, ou se a opção "Erros padrão robustos" tiver sido selecionada, será usada a matriz de covariância QML (White). Outras possibilidades podem ser especificadas usando o comando <@ref="set"> (por exemplo a matriz de informação, ou o estimador Bollerslev–Wooldridge ). 

A variância condicional estimada, juntamente com os resíduos e outras estatísticas do modelo, podem ser acedidas e acrescentadas ao modelo usando o menu "Gravar" na janela onde o modelo é apresentado. Se a opção "Normalizar os resíduos" estiver selecionada, os resíduos são divididos pela raiz quadrada da variância condicional. 

Menu path: /Modelo/Série temporal/GARCH

Script command: <@ref="garch">

# genr Dataset "Gerar uma nova variável"

Use esta caixa de txto para definir uma nova variável, seguindo o padrão <@var="nome"> = <@var="expressão">. A expressão deve ser uma combinação bem construída de nomes de variáveis, constantes, operadores e funções (descrito adiante). Para garantir que o tipo de variável criada é o desejado, você pode anteceder o nome com o tipo, como sejam, <@lit="scalar">, <@lit="series"> ou <@lit="matrix">. Por exemplo, para criar uma série que tenha um valor constante de 10, você pode escrever 

<code>          
   series c = 10
</code>

(de outro modo <@lit="c = 10"> resultaria numa variável escalar). 

Os <@itl="operadores aritméticos"> suportados são, por ordem de precedência: <@lit="^"> (potenciação); <@lit="*">, <@lit="/"> e <@lit="%"> (resto da divisão inteira); <@lit="+"> e <@lit="-">. 

Os <@itl="operadores Booleanos"> são (mais uma vez, por ordem de precedência): <@lit="!"> (negação), <@lit="&&"> (E lógico), <@lit="||"> (OU lógico), <@lit=">">, <@lit="<">, <@lit="=">, <@lit=">="> (maior ou igual), <@lit="<="> (menor ou igual) e <@lit="!="> (diferente). Os operadores Booleanos podem ser usados na construção de variáveis auxiliares ('dummy'): por exemplo <@lit="(x > 10)"> retorna 1 se <@lit="x"> > 10, 0 caso contrário. 

As constantes pré-definidas são <@lit="pi"> e <@lit="NA">. Esta última representa um valor omisso: você pode inicializar uma variável como tendo um valor omisso com <@lit="scalar x = NA">. 

O comando <@lit="genr"> suporta uma larga gama de funções matemáticas e estatísticas, incluindo, para além das usuais, várias que são especialmente dedicadas à econometria. Adicionalmente oferece acesso a numerosas variáveis internas que são definidas no decorrer das regressões, testes de hipóteses e outros. Para uma lista de funções e acessores, ver <@gfr="the Gretl function reference">. 

Para além dos operadores e funções mencionados acima, existem alguns usos especiais de <@lit="genr">: 

<indent>
• <@lit="genr time"> cria uma variável de tendência temporal (1,2,3,…) com o nome <@lit="time">. <@lit="genr index"> faz a mesma coisa exceto em que o nome da variável é <@lit="index">. 
</indent>

<indent>
• <@lit="genr dummy"> cria variáveis auxiliares ('dummy') até à periodicidade dos dados. No caso de dados trimestrais (periodicidade 4), o programa cria <@lit="dq1"> = 1 para o primeiro trimestre 0 nos outros timestres, <@lit="dq2"> = 1 para o segundo trimestre e 0 para os outros trimestres, e por aí adiante. No caso de dados mensais as variáveis auxiliares têm os nomes <@lit="dm1">, <@lit="dm2">, e por aí adiante. No caso de outras frequências os nomes são <@lit="dummy_1">, <@lit="dummy_2">, etc. 
</indent>

<indent>
• <@lit="genr unitdum"> e <@lit="genr timedum"> criam conjuntos de variáveis auxiliares especiais para usar com dados de painel. O primeiro codifica para as seções-cruzadas e o segundo para os períodos temporais das observações. 
</indent>

<@itl="Nota">: No programa de linha-de-comandos, os comandos <@lit="genr"> que obtenham dados de modelo referem-se sempre ao modelo que foi estimado mais recentemente. Isto também é válido para o programa em ambiente gráfico (GUI), ao se usar <@lit="genr"> na "consola gretl" ou ao introduzir uma expressão usando "Definir nova variável" no menu Acrescentar na janela principal. No entanto, no GUI, você tem a possibilidade de obter dados a partir de qualquer modelo que esteja disponível numa janela (independentemente se é ou não o modelo mais recente). Isso é feito no menu "Gravar" na janela do modelo. 

A variável especial <@lit="obs"> serve como um índice das observações. Por exemplo <@lit="genr dum = (obs=15)"> irá gerar uma variável auxiliar que tem valor 1 para a observação 15 e 0 para as outras. Você também pode usar esta variável para escolher certas observações por data ou nome. Por exemplo, <@lit="genr d = (obs>1986:4)">, <@lit="genr d = (obs>"2008/04/01")">, ou <@lit="genr d = (obs="CA")">. Se se usarem datas diárias ou etiquetas neste contexto, elas devem ser indicadas dentro de aspas. Datas trimestrais ou mensais (com um dois-pontos) podem ser usadas sem aspas. Note que no caso de dados de séries temporais anuais, o ano não se distingue sintáticamente de um simples inteiro; como tal, se você quiser comparar observações com <@lit="obs"> por ano, você tem que usar a função <@lit="obsnum"> para converter o ano para um valor de índice iniciado em 1, tal como em <@lit="genr d = (obs>obsnum(1986))">. 

Valores escalares podem ser extraídos de uma série no contexto de uma expressão <@lit="genr">, usando a sintaxe <@var="varname"><@lit="["><@var="obs"><@lit="]">. O valor <@var="obs"> pode ser dado por núnmero ou data. Exemplos: <@lit="x[5]">, <@lit="CPI[1996:01]">. Para dados diários, deve se usar a forma <@var="YYYY/MM/DD">, por exemplo, <@lit="ibm[1970/01/23]">. 

Uma observação individual numa série pode ser modificado usando <@lit="genr">. Para fazer isto, uma observação válida numérica ou de data, tem que ser acrescentada dentro de parentesis rectos, ao nome da variável no lado esquerdo da expressão. Por exemplo, <@lit="genr x[3] = 30"> ou <@lit="genr x[1950:04] = 303.7">. 

Menu path: /Acrescentar/Definir nova variável
Other access: Menu de contexto da janela principal

Script command: <@ref="genr">

# genrand Programming "Gerador de variáveis aleatórias"

Neste diálogo você tem que fornecer um nome para a variável a ser criada, mais alguma informação adicional depedendo da distribuição. 

<indent>
• Uniforme: os limites inferior e superior para a distribuição. 
</indent>

<indent>
• Normal: a média e o desvio padrão (positivo). 
</indent>

<indent>
• Qui-quadrado e t de Student: os graus de liberdade, que têm que ser positivos. 
</indent>

<indent>
• F: numerador, denonimador e o grau de liberdade. 
</indent>

<indent>
• gama: parâmetros de forma e de escala (ambos positivos). 
</indent>

<indent>
• Binomial: a probabilidade de "sucesso" e o inteiro positivo para o número de experiências. 
</indent>

<indent>
• Poisson: a média positiva (que também é igual à variância). 
</indent>

Se você quiser gerar sequências repetíveis de números pseudo-aleatórios, você pode definir a semente no menu Ferramentas. 

# genseed Programming "Definindo a semente dos números aleatórios"

A "semente" controla o ponto inicial da sequência dos números pseudo-aleatórios gerados num dada sessão gretl. Por omissão a semente é definida usando a hora do sistema, quando o programa é iniciado. Isto garante que você obtém uma sequência diferente de número aleatórios de cada vez que iniciar o programa. Se você quiser obter sequências repetíveis, você precisa de definir manualmente a semente (tomando nota do valor utilizado). 

Note que sempre que você clicar "OK" nesta janela de diálogo, o gerador é reiniciado usando a semente fornecida. Portanto, por exemplo, se você (a) definir a semente para (digamos) 147; (b) gerar uma série com distribuição normal; (c) voltar a esta janela de diálogo e clicar "OK" outra vez com a semente ainda a 147; e depois (d) gerar uma segunda série com a distribuição normal, as duas séries serão iguais. 

# gmm Estimation "Estimação GMM"

Faz estimação usando o Método dos Momentos Generalizado, 'Generalized Method of Moments' (GMM) com o algoritmo BFGS (Broyden, Fletcher, Goldfarb, Shanno). Você tem que especificar um ou mais comandos para a atualização das quantidades relevantes (tipicamente os resíduos GMM), um ou mais conjuntos das condições, uma matriz inicial dos pesos, e uma listagem dos parâmetros a serem estimados, tudo entre os marcadores <@lit="gmm"> e <@lit="end gmm">. Quaisquer opções devem ser acrescentadas à linha <@lit="end gmm"> . 

Por favor veja mais detalhes sobre este comando em <@pdf="the Gretl User's Guide">. Aqui apenas ilustramos com um exemplo simples. 

<code>          
   gmm e = y - X*b
     orthog e ; W
     weights V
     params b
   end gmm
</code>

No exemplo acima nós assumimos que <@lit="y"> e <@lit="X"> são matrizes, <@lit="b"> é um vetor de tamanho apropriado dos valores dos parâmetros, <@lit="W"> é a matriz dos instrumentos, e <@lit="V"> é uma matriz adequada de pesos. A declaração 

<code>          
   orthog e ; W
</code>

indica que o vetor dos resíduos <@lit="e"> é em princípio ortognal a cada um dos instromentos que compõem as colunas de <@lit="W">. 

Menu path: /Modelo/GMM

Script command: <@ref="gmm">

# graphing Graphs "Representação Gráfica"

Gretl chama um programa separado, designadamente gnuplot, para gerar gráficos. Gnuplot é um programa de gráficos completo com muitas de opções. Gretl dá-lhe acesso direto, por intermédio de um interface gráfico, a um subconjunto destas opções e tenta escolher valores adequados para si; também permite-lhe controlar completamente os detalhes dos gráficos se assim o desejar. 

Com um gráfico apresentado, você pode clicar na janela do gráfico para aceder a um menu de contexto com as seguintes opções: 

<indent>
• Gravar como 'postscript' (EPS): grava o gráfico no formato 'postscript' encapsulado (EPS) 
</indent>

<indent>
• Gravar como PNG: grava o gráfico no formato 'Portable Network Graphics' 
</indent>

<indent>
• Gravar como PDF: grava o gráfico no formato 'Portable Document Format' 
</indent>

<indent>
• Gravar como metaficheiro do Windows (EMF): grava o gráfico no formato 'Enhanced Metafile Format', a cores ou monocromático 
</indent>

<indent>
• Guardar para a sessão como ícone: o gráfico aparecerá na forma de ícone quando você selecionar "Ver/Por Ícones" do menu da janela principal 
</indent>

<indent>
• Ampliar: deixa você selecionar uma área dentro do gráfico para melhor inspeção. Depois pode substituir o gráfico inicial ou restaurar a vista normal. 
</indent>

<indent>
• Imprimir: (apenas no ambiente gráfico Gnome e no MS Windows) permite-lhe imprimir o gráfico diretamente 
</indent>

<indent>
• Copia para a memória de edição: (apenas no MS Windows) permite-lhe colar o gráfico em aplicações Windows como seja o MS Word 
</indent>

<indent>
• Editar: abre uma janela de controlo do gráfico que lhe permite ajustar os vários aspetos da sua aparência 
</indent>

<indent>
• Fechar: fecha a janela do gráfico 
</indent>

Se você conhece algo sobre o gnuplot e deseja obter melhor controlo sobre a aparência sobre o gráfico do que a que é disponibilizada no controlador gráfico ("Editar" option), você tem outras opção: 

<indent>
• Assim que o gráfico é guardado para a sessão como ícone, você pode clicar com o botão direito sobre o ícone para aceder a um menu de contexto. Uma das opções é "Editar comandos do gráfico", o que abre uma janela de edição com os comandos gnuplot utilizados nesse gráfico. Você pode editar esses comandos e gravá-los para uso futuro ou enviar para o gnuplot (com o ícone de execução na barra de ferramentas da janela de edição). 
</indent>

Para saber mais sobre gnuplot, ver http://www.gnuplot.info 

# graphpg Graphs "Página dos gráficos de Gretl"

A "página dos gráficos" de sessão apenas funcionará se você tiver instalado o sistema de produção de texto LaTeX, e puder gerar e visionar documentos PDF ou PostScript. 

Na janela de sessão por ícones, você pode arrastar até oito gráficos para dentro de um ícone de página de gráficos. Quando você fizer duplo-clique na página de gráficos (ou com o botão direito e selecionar "Mostrar"), será produzida uma página com os gráficos selecionados e apresentada no visionador adequado. A partir deste você poderá imprimir a página. 

Para limpar a página de gráficos, clicar com o botão direito no seu ícone e selecionar "Limpar". 

Note que em sistemas diferentes do MS Windows, você pode ter que ajustar as definições dos programas usados para visionar documentos PDF ou PostScript. Isso encontra-se dentro do separador "Programas" na janela de diálogo das Preferências do gretl (a partir do menu Ferramentas da janela principal). 

Também é possível trabalhar com a página de gráficos a partir se sequência de comandos, ou usando a consola (dentro do programa em ambiente gráfico). São suportados os seguintes comandos e opções: 

Para acrescentar um gráfico à página de gráficos, dê o comando <@lit="graphpg add"> depois de o ter gravado como um gráfico com nome, tal como 

<code>          
   grf1 <- gnuplot Y X
   graphpg add
</code>

Para ver a página de gráficos: <@lit="graphpg show">. 

Para limpar a página de gráficos: <@lit="graphpg free">. 

Para ajustar a escala da fonte usada na página de gráficos, use <@lit="graphpg fontscale"> <@var="escala">, onde <@var="escala"> é um multiplicador (com o valor 1,0 por omissão). Assim, para tornar a o fonte 50 porcento maior que a por inicial você pode 

<code>          
   graphpg fontscale 1.5
</code>

Para chamar a impressão da página de gráficos para um ficheiro, use a opção <@opt="--⁠output="> mais um nome de ficheiro; o nome do ficheiro deverá ter o sufixo "<@lit=".pdf">", "<@lit=".ps">" ou "<@lit=".eps">". Por exemplo: 

<code>          
   graphpg --output="meu_ficheiro.pdf"
</code>

Neste contexto, por omissão o resultado usa linhas coloridas; para usar padrões ponto/traço em vez de cores, você pode acrescentar a opção <@opt="--⁠monochrome">. 

Script command: <@ref="graphpg">

# 3-D Graphs "Gráficos tridimensionais"

Caso você tenha instalado o gnuplot 3.8 ou superior, pode pode aproveitar a funcionalidade de manipulação de gráficos 3-D com o rato (rodá-los e expandir ou encolher os eixos). 

Ao criar um gráfico 3-D, note que o eixo Z será mostrado como sendo o eixo vertical. Como tal, se você tiver alguma variável dependente que pense poder ser influenciada por duas variáveis independentes, você deve colocar a variável dependente no eixo Z, e as variáveis independentes nos eixos X e Y. 

Contrariamente à maior parte dos gráficos de gretl, os gráficos 3-D são controlados por gnuplot e não pelo gretl. O menu de edição de gráficos do gretl não estará disponível. 

# gui-funcs Programming "Funções especiais"

Este diálogo permite-lhe especificar quais as funções de um pacote, deverão ter certos papéis especiais. Note que uma certa função pode ter apenas um dos seguintes papéis, e para se qualificarem como candidatas para um destes papéis as funções tem que satisfazer certos critérios. 

<indent>
• <@lit="bundle-print">: imprime saídas baseadas no conteúdo de um "bundle" produzido pelo seu pacote. Critério: esta função tem que ter como o seu primeiro parâmetro um ponteiro-"bundle". Se estiver presente um segundo parâmetro ele tem que tomar a forma de seletor inteiro que tenha um valor por omissão. 
</indent>

<indent>
• <@lit="bundle-plot">: produz um ou mais gráficos usando um "bundle" produzido pelo seu pacote. Critério: o mesmo que para <@lit="bundle-print">. 
</indent>

<indent>
• <@lit="bundle-test">: efetua algum tipo de teste estatístico usando um "bundle" produzido pelo seu pacote. Critério: o mesmo que para <@lit="bundle-print">. 
</indent>

<indent>
• <@lit="gui-main">: o interface que deve ser apresentado aos usuários por omissão quando usado em modo gráfico (GUI). Isto é útil apenas quando um pacote tem mais que um interface público. 
</indent>

<indent>
• <@lit="gui-precheck">: função guardião que retorna 0 se a funcionalidade do seu pacote é aplicável no contexto corrente, e não-zero caso contrário. Isto destina-se a ser utilizado com pacotes que operam num modelo de algum modo, para descartar tipos de modelos que não são suportados pelo pacote. 
</indent>

# gui-htest Tests "Calculador de Estatísticas de Teste"

O calculador de estatísticas de teste do gretl determina estatísticas de teste e valores p para diversos testes de hipóteses para uma ou duas populações. As entradas necessárias tomam a forma de estatísticas amostrais derivadas de uma ou duas amostras, dependendo do teste escolhido. Estas estatísticas podem ser indicadas como valores numéricos. Em alternativa, se você tiver um conjunto de dados aberto, você pode usar gretl para calcular estatísticas amostrais para uma ou mais variáveis selecionadas (no caso de médias e variâncias, mas não no caso de proporções). 

Se você quiser basear o seu teste numa variável do conjunto de dados, primeiro ative esta opção selecionando a caixa intitulada "Usar variável do conjunto de dados". De seguida o seletor de variáveis fica ativo e você pode selecionar a variável. Quando seleciona uma variável a partir da lista, as estatísticas relevantes são automaticamente colocadas nas caixas abaixo. 

Adicionalmente, além da simples seleção de uma variável, você tem a opção de especificar uma restrição para a variável selecionada (isto é, definir uma sub-amostra). Por exemplo, suponhamos que você tem informação de salários na variável "salario" e que também tem uma variável auxiliar chamada "sexo" que é igual a 1 para masculino e 0 para feminino (ou vice versa). Então, no teste para as diferenças de duas médias, você pode selecionar "salario" em ambos os campos, mas acrescentar ao campo do topo "(sexo=0)" e ao de baixo "(sexo=1)". Isto resultaria num teste para as diferenças entre rendimentos dos homens e rendimentos das mulheres. Note que quando você usa uma restrição desta forma, você tem que pressionar a tecla "Enter" para obter o cálculo das estatísticas amostrais. 

A restrição de sub-amostragem pode ser colocada dentro de parêntesis após a variável selecionada, e em geral a restrição toma a forma "var2 op val", onde var2 é o nome de uma variável no conjunto de dados corrente, val é um valor numérico, e op é um operador de comparação escolhido dentre =, !=, <, >, <= ou >= (respetivamente igualdade, desigualdade, menor que, maior que, menor ou igual, e maior ou igual). Os espaços separando o operador são opcionais. 

# gui-htest-np Tests "Testes Não-paramétricos"

No separador "Teste da Diferença" você pode efetuar o teste não-paramétrico para a diferença entre duas populações ou grupos, o teste específico depende da opção selecionada. 

Teste dos sinais: Este teste é baseado no fato de que se duas amostras, <@itl="x"> e <@itl="y">, são obtidas aleatoriamente a partir da mesma distribuição, a probabilidade de <@itl="x"><@sub="i"> > <@itl="y"><@sub="i">, para cada observação <@itl="i">, deve ser igual a 0,5. A estatística de teste é <@itl="w">, o número de observações em que <@itl="x"><@sub="i"> > <@itl="y"><@sub="i">. Sob a hipótese nula isto segue uma distribuição Binomial com parâmetros (<@itl="n">; 0,5), onde <@itl="n"> é o número de observações. 

Teste ordinal da soma de Wilcoxon: É executado o teste ordinal da soma de Wilcoxon. Este teste inicia por ordenar as observações da amostra conjunta do menor para o maior, depois obtém a soma das ordens para uma das amostras. As duas amostras não têm que ser da mesma dimensão, e se elas forem diferentes será usada a de menor tamanho para calcular a soma das ordens. Sob a hipótese nula de que as amostras foram obtidas a partir de populações com igual mediana, a distribuição de probabilidade da soma de ordens pode ser calculada para quaisquer dimensões de amostras; e para amostras suficientemente grandes existe uma aproximação à Normal. 

Teste ordinal dos sinais de Wilcoxon: É executado o teste ordinal dos sinais de Wilcoxon. Isto é designado pelo emparelhamento de dados de modo a que, por exemplo, os valores de uma variável de uma amostra de indivíduos antes e depois de algum tratamento. O teste inicia por encontrar as diferenças entre as observações emparelhadas, <@itl="x"><@sub="i"> – <@itl="y"><@sub="i">, ordenando estas diferenças por valor absoluto, depois atribuindo a cada par uma ordem com sinal, este sinal concorda com o sinal da diferença. Em seguida é calculado <@itl="W"><@sub="+">, a soma das ordens com sinal positivo. Tal como no teste ordinal da soma, esta estatística tem uma distribuição bem-definida sob a hipótese nula de que a diferença de medianas é zero, o que converge para a Normal em amostras de tamanho razoável. 

No separador "Teste de Aleatoriedade" você pode efetuar um teste para a aleatoriedade de uma dada variável, baseada no número de sequências consecutivas de valores positivos ou negativos. Se você selecionar a opção "Usar a primeira diferença", a variável é diferenciada antes da análise e como tal as sequências são interpretadas como sendo sequências crescentes ou decrescentes de valores da variável original. A estatística de teste é baseada na aproximação normal à distribuição do número de sequências segundo a nulidade de aleatoriedade. 

# hausman Tests "Diagnósticos de Painel"

Este teste apenas está disponível após e estimar um modelo de mínimos quadrados (OLS) usando dados de painel (ver também <@lit="setobs">). Ele testa o modelo de amostragem simples ("pooled") contra as alternativas principais, os modelos de efeitos fixos e efeitos aleatórios. 

O modelo de efeitos fixos permite variar a interseção da regressão ao longo das unidades de seção cruzada. Uma estatística teste-<@itl="F"> é apresentada segundo a hipótese nula de que as interseções não diferem. O modelo de efeitos aleatórios decompõe a variância dos resíduos em duas partes, uma parte específica à unidade de seção cruzada e outra específica para a observação em particular. (Este estimador pode ser calculado apenas se o número de unidades de seção cruzada nos dados exceder o número de parâmetros a serem estimados.) A estatística de teste Breusch–Pagan LM, testa a hipótese nula de que o estimador mínimos quadrados de amostragem ("pooled") é adequado em oposição ao da alternativa de efeitos aleatórios. 

O modelo mínimos quadrados de amostragem ("pooled") pode ser rejeitado contra ambas as alternativas, efeitos fixos e efeitos aleatórios. Desde que o erro específico por unidade ou por grupo seja não correlacionado com as variáveis independentes, o estimador de efeitos aleatórios é mais eficiente do que o estimador de efeitos fixos; caso contrário o estimador de efeitos aleatórios é inconsistente e o estimador de efeitos fixos será preferido. A hipótese nula para o teste de Hausman é de que o erro específico de grupo não é tão correlacionado (e como tal o modelo de efeitos aleatórios é preferível). Um valor p baixo para este teste conta contra o modelo de efeitos aleatórios e a favor do modelo de efeitos fixos. 

Menu path: Janela do modelo, /Testes/Diagnósticos de Painel

Script command: <@ref="hausman">

# hccme Estimation "Erros padrão robustos"

Você tem disponível várias variantes de cálculo para erros padrão que são robustos na presença de heteroscedasticidade (e, no caso do estimador HAC, autocorrelação). 

HC0 produz os "erros padrão de White" originais; HC1, HC2, HC3 e HC3a são variações subsequentes que em geral são reconhecidas de produzirem melhores (mais fiáveis) resultados. Para detalhes dos estimadores, ver <@bib="MacKinnon and White (Journal of Econometrics, 1985);mackinnon-white85"> ou <@bib="Davidson and MacKinnon, Econometric Theory and Methods (Oxford, 2004);davidson-mackinnon04">. As etiquetas aqui usadas são as que Davidson e MacKinnon usaram. A variante "HC3a" é o canivete ("jackknife"), como descrita em MacKinnon e White; HC3 é uma boa aproximação ao canivete. 

Se você usar o estimador HAC para OLS em dados de séries temporais, você pode afinar o comprimento de desfasamento usando o comando <@lit="set">. Por favor, para mais detalhes, ver o manual do arquivo de ajuda das sequências de comandos. 

Ao se estimar um modelo OLS usando dados de painel, o estimador robusto por omissão da matriz de covariância é o dado por Arellano. A alternativa são os Erros Padrão Corrigidos de Painel de Beck e Katz (PCSE). Este tem em conta a heteroscedasticidade mas não a autocorrelação. 

Para modelos GARCH são fornecidos dois estimadores robustos para a matriz de covariância: o estimador de Verosimilhança Quasi-Maximum (QML), e estimador de Bollerslev-Wooldridge (BW). 

# hsk Estimation "Heteroskedasticity-corrected estimates"

This command is applicable where heteroskedasticity is present in the form of an unknown function of the regressors which can be approximated by a quadratic relationship. In that context it offers the possibility of consistent standard errors and more efficient parameter estimates as compared with OLS. 

The procedure involves (a) OLS estimation of the model of interest, followed by (b) an auxiliary regression to generate an estimate of the error variance, then finally (c) weighted least squares, using as weight the reciprocal of the estimated variance. 

In the auxiliary regression (b) we regress the log of the squared residuals from the first OLS on the original regressors and their squares. The log transformation is performed to ensure that the estimated variances are non-negative. Call the fitted values from this regression <@itl="u"><@sup="*">. The weight series for the final WLS is then formed as 1/exp(<@itl="u"><@sup="*">). 

Menu path: /Model/Other linear models/Heteroskedasticity corrected

Script command: <@ref="hsk">

# hurst Statistics "Hurst exponent"

Calculates the Hurst exponent (a measure of persistence or long memory) for a time-series variable having at least 128 observations. 

The Hurst exponent is discussed by Mandelbrot. In theoretical terms it is the exponent, <@itl="H">, in the relationship 

  <@fig="hurst">

where RS is the "rescaled range" of the variable <@itl="x"> in samples of size <@itl="n"> and <@itl="a"> is a constant. The rescaled range is the range (maximum minus minimum) of the cumulated value or partial sum of <@itl="x"> over the sample period (after subtraction of the sample mean), divided by the sample standard deviation. 

As a reference point, if <@itl="x"> is white noise (zero mean, zero persistence) then the range of its cumulated "wandering" (which forms a random walk), scaled by the standard deviation, grows as the square root of the sample size, giving an expected Hurst exponent of 0.5. Values of the exponent significantly in excess of 0.5 indicate persistence, and values less than 0.5 indicate anti-persistence (negative autocorrelation). In principle the exponent is bounded by 0 and 1, although in finite samples it is possible to get an estimated exponent greater than 1. 

In gretl, the exponent is estimated using binary sub-sampling: we start with the entire data range, then the two halves of the range, then the four quarters, and so on. For sample sizes smaller than the data range, the RS value is the mean across the available samples. The exponent is then estimated as the slope coefficient in a regression of the log of RS on the log of sample size. 

Menu path: /Variable/Hurst exponent

Script command: <@ref="hurst">

# intreg Estimation "Interval regression model"

Estimates an interval regression model. This model arises when the dependent variable is imperfectly observed for some (possibly all) observations. In other words, the data generating process is assumed to be 

  <@itl="y* = x b + u">

but we only observe <@itl="m <= y* <= M"> (the interval may be left- or right-unbounded). Note that for some observations <@itl="m"> may equal <@itl="M">. The variables <@var="minvar"> and <@var="maxvar"> must contain <@lit="NA">s for left- and right-unbounded observations, respectively. 

In the model specification dialog, <@var="minvar"> and <@var="maxvar"> are indentified as the Lower bound variable and the Upper bound variable respectively. 

The model is estimated by maximum likelihood, assuming normality of the disturbance term. 

By default, standard errors are computed using the negative inverse of the Hessian. If the "Robust standard errors" box is checked, then QML or Huber–White standard errors are calculated instead. In this case the estimated covariance matrix is a "sandwich" of the inverse of the estimated Hessian and the outer product of the gradient. 

Menu path: /Model/Limited dependent variable/Interval regression

Script command: <@ref="intreg">

# irfboot Graphs "Impulse response plots"

If you select the bootstrap option when plotting impulse responses, gretl computes a confidence interval for the responses using the bootstrap method. The residuals from the original VAR (or VECM) are resampled with replacement; an artificial dataset is constructed based on the original parameter estimates and the resampled residuals; the system is re-estimated and the impulse responses are re-evaluated. This is repeated 999 times and the α/2 and 1 – α/2 quantiles for the responses are found and plotted along with the point estimates. This option is not currently available for restricted VECMs. 

This dialog also supports reordering of the variables for the Cholesky decomposition of the cross-equation covariance matrix. The default is given by the order in which the variables are entered into the model specification, but the up and down arrows can be used to promote or demote a selected variable. 

# kalman Estimation "Kalman filter"

Opens a block of statements to set up a Kalman filter. This block should end with the line <@lit="end kalman">, to which the options shown above may be appended. The intervening lines specify the matrices that compose the filter. For example, 

<code>          
   kalman 
     obsy y
     obsymat H
     statemat F
     statevar Q
   end kalman
</code>

Please see <@pdf="the Gretl User's Guide"> for details. 

See also <@xrf="kfilter">, <@xrf="ksimul">, <@xrf="ksmooth">. 

Script command: <@ref="kalman">

# kpss Tests "KPSS stationarity test"

Computes the KPSS test (Kwiatkowski, Phillips, Schmidt and Shin, Journal of Econometrics, 1992) for stationarity of the given variable (or its first difference, if the differencing option is selected). The null hypothesis is that the variable in question is stationary, either around a level or, if the "include a trend" box is checked, around a deterministic linear trend. 

The selected lag order determines the size of the window used for Bartlett smoothing. If the "show regression results" box is checked the results of the auxiliary regression are printed, along with the estimated variance of the random walk component of the variable. 

The critical values shown for the test statistic are based on the response surfaces estimated by <@bib="Sephton (Economics Letters, 1995);sephton95">, which are more accurate for small samples than the values given in the original KPSS article. When the test statistic lies between the 10 percent and 1 percent critical values a p-value is shown; this is obtained by linear interpolation and should not be taken too literally. 

Menu path: /Variable/Unit root tests/KPSS test

Script command: <@ref="kpss">

# lad Estimation "Least Absolute Deviation estimation"

Calculates a regression that minimizes the sum of the absolute deviations of the observed from the fitted values of the dependent variable. Coefficient estimates are derived using the Barrodale–Roberts simplex algorithm; a warning is printed if the solution is not unique. 

Standard errors are derived using the bootstrap procedure with 500 drawings. The covariance matrix for the parameter estimates, printed when the <@opt="--⁠vcv"> flag is given, is based on the same bootstrap. 

Menu path: /Model/Robust estimation/Least Absolute Deviation

Script command: <@ref="lad">

# lags-dialog Estimation "Lag selection box"

In this dialog you can select the lag order for the independent variables in a time-series model, and in some cases for the dependent variable also. (But note that the common lag order for vector models such as VARs and VECMs is handled separately, via a selection spinner in the main model dialog box.) 

The spinners on the left let you select a range of consecutive lags for any given variable. To specify non-consecutive lags, click the check box next to the entry field titled "specific lags". This activates the entry box, into which you can type a list of lags, separated by spaces. 

The row marked "default" offers a quick way to set a common lag specification for all the independent variables: values set in that row are copied to all the others (apart from the dependent variable, if present). 

The dependent variable is treated specially: the minimum lag must be zero, which places the current value of the variable on the left-hand side of the model. Any higher lags appear with the independent variables on the right-hand side of the model. 

Values selected in this dialog are remembered for the duration of your session with a given dataset. 

# leverage Tests "Influential observations"

Must follow an <@lit="ols"> command. Calculates the leverage (<@itl="h">, which must lie in the range 0 to 1) for each data point in the sample on which the previous model was estimated. Displays the residual (<@itl="u">) for each observation along with its leverage and a measure of its influence on the estimates, <@itl="uh">/(1 – <@itl="h">). "Leverage points" for which the value of <@itl="h"> exceeds 2<@itl="k">/<@itl="n"> (where <@itl="k"> is the number of parameters being estimated and <@itl="n"> is the sample size) are flagged with an asterisk. For details on the concepts of leverage and influence see <@bib="Davidson and MacKinnon (1993);davidson-mackinnon93">, Chapter 2. 

DFFITS values are also computed: these are "studentized residuals" (predicted residuals divided by their standard errors) multiplied by <@fig="dffit">. For discussions of studentized residuals and DFFITS see chapter 12 of <@bib="Maddala's Introduction to Econometrics;maddala92"> or <@bib="Belsley, Kuh and Welsch (1980);belsley-etal80">. 

Briefly, a "predicted residual" is the difference between the observed value of the dependent variable at observation <@itl="t">, and the fitted value for observation <@itl="t"> obtained from a regression in which that observation is omitted (or a dummy variable with value 1 for observation <@itl="t"> alone has been added); the studentized residual is obtained by dividing the predicted residual by its standard error. 

The "+" icon at the top of the leverage test window brings up a dialog box that allows you to save one or more of the test variables to the current data set. 

After execution, the <@lit="$test"> accessor returns the cross-validation criterion, which is defined as the sum of squared deviations of the dependent variable from its forecast value, the forecast for each observation being based on a sample from which that observation is excluded. (This is known as the <@itl="leave-one-out"> estimator). For a broader discussion of the cross-validation criterion, see Davidson and MacKinnon's <@itl="Econometric Theory and Methods">, pages 685–686, and the references therein. 

Menu path: Model window, /Tests/Influential observations

Script command: <@ref="leverage">

# levinlin Tests "Levin-Lin-Chu test"

Carries out the panel unit-root test described by <@bib="Levin, Lin and Chu (2002);LLC2002">. The null hypothesis is that all of the individual time series exhibit a unit root, and the alternative is that none of the series has a unit root. (That is, a common AR(1) coefficient is assumed, although in other respects the statistical properties of the series are allowed to vary across individuals.) 

Menu path: /Variable/Unit root tests/Levin-Lin-Chu test

Script command: <@ref="levinlin">

# loess Estimation "Loess"

Performs locally-weighted polynomial regression and produces a series containing predicted values of the dependent variable for each non-missing value of the independent variable. The method is as described by <@bib="William Cleveland (1979);cleveland79">. 

The controls allow you to specify the order of the polynomial in the independent variable and the proportion of the data points to be used in each local regression (the bandwidth). Higher values of the bandwidth produce a smoother outcome. 

If the robust weights box is checked the local regression procedure is iterated twice, with the weights being modified based on the residuals from the previous iteration so as to give less influence to outliers. 

# logistic Estimation "Logistic regression"

Logistic regression: carries out an OLS regression using the logistic transformation of the dependent variable, 

  <@fig="logistic1">

The dependent variable must be strictly positive. If all its values lie between 0 and 1, the default is to use a <@itl="y"><@sup="*"> value (the asymptotic maximum of the dependent variable) of 1; if its values lie between 0 and 100, the default <@itl="y"><@sup="*"> is 100. 

You may specify a different maximum <@itl="y"> value. Note that the supplied value must be greater than all of the observed values of the dependent variable. 

The fitted values and residuals from the regression are automatically transformed using 

  <@fig="logistic2">

where <@itl="x"> represents either a fitted value or a residual from the OLS regression using the transformed dependent variable. The reported values are therefore comparable with the original dependent variable. 

Note that if the dependent variable is binary, you should use the <@ref="logit"> command instead. 

Menu path: /Model/Limited dependent variable/Logistic

Script command: <@ref="logistic">

# logit Estimation "Logit regression"

If the dependent variable is a binary variable (all values are 0 or 1) maximum likelihood estimates of the coefficients on <@var="indepvars"> are obtained via the Newton–Raphson method. As the model is nonlinear the slopes depend on the values of the independent variables. By default the slopes with respect to each of the independent variables are calculated (at the means of those variables) and these slopes replace the usual p-values in the regression output. This behavior can be suppressed my giving the <@opt="--⁠p-values"> option. The chi-square statistic tests the null hypothesis that all coefficients are zero apart from the constant. 

By default, standard errors are computed using the negative inverse of the Hessian. If the "Robust standard errors" box is checked, then QML or Huber–White standard errors are calculated instead. In this case the estimated covariance matrix is a "sandwich" of the inverse of the estimated Hessian and the outer product of the gradient. See chapter 10 of Davidson and MacKinnon for details. 

If the dependent variable is not binary but is discrete, then by default it is interpreted as an ordinal response, and Ordered Logit estimates are obtained. However, if the <@opt="--⁠multinomial"> option is given, the dependent variable is interpreted as an unordered response, and Multinomial Logit estimates are produced. (In either case, if the variable selected as dependent is not discrete an error is flagged.) In the multinomial case, the accessor <@lit="$mnlprobs"> is available after estimation, to get a matrix containing the estimated probabilities of the outcomes at each observation (observations in rows, outcomes in columns). 

If you want to use logit for analysis of proportions (where the dependent variable is the proportion of cases having a certain characteristic, at each observation, rather than a 1 or 0 variable indicating whether the characteristic is present or not) you should not use the <@lit="logit"> command, but rather construct the logit variable, as in 

<code>          
   series lgt_p = log(p/(1 - p))
</code>

and use this as the dependent variable in an OLS regression. See chapter 12 of <@bib="Ramanathan (2002);ramanathan02">. 

Menu path: /Model/Limited dependent variable/Logit

Script command: <@ref="logit">

# mahal Statistics "Mahalanobis distances"

Computes the Mahalanobis distances between the series in <@var="varlist">. The Mahalanobis distance is the distance between two points in a <@itl="k">-dimensional space, scaled by the statistical variation in each dimension of the space. For example, if <@itl="p"> and <@itl="q"> are two observations on a set of <@itl="k"> variables with covariance matrix <@itl="C">, then the Mahalanobis distance between the observations is given by 

  <@fig="mahal">

where (<@itl="p"> – <@itl="q">) is a <@itl="k">-vector. This reduces to Euclidean distance if the covariance matrix is the identity matrix. 

The space for which distances are computed is defined by the selected variables. For each observation in the current sample range, the distance is computed between the observation and the centroid of the selected variables. This distance is the multidimensional counterpart of a standard <@itl="z">-score, and can be used to judge whether a given observation "belongs" with a group of other observations. 

If the number of variables selected is 4 or less, the covariance matrix and its inverse are printed. Clicking the "+" button at the top of the window displaying the distances give you the option of adding the distances to the dataset as a new variable. 

Menu path: /View/Mahalanobis distances

Script command: <@ref="mahal">

# meantest Tests "Difference of means"

By default the test statistic is calculated on the assumption that the variances are equal for the two variables; with the <@opt="--⁠unequal-vars"> option the variances are assumed to be different. This will make a difference to the test statistic only if there are different numbers of non-missing observations for the two series. 

Calculates the t statistic for the null hypothesis that the population means are equal for two selected series, and shows its p-value. The command may be called with or without the assumption that the variances are equal for the two variables (although this will make a difference to the test statistic only if there are different numbers of non-missing observations for the two series.) 

Menu path: /Tools/Test statistic calculator

Script command: <@ref="meantest">

# missing Dataset "Missing data values"

Set a numerical value that will be interpreted as "missing" or "not available", either for a particular data series (under the Variable menu) or globally for the entire data set (under the Sample menu). 

Gretl has its own internal coding for missing values, but sometimes imported data may employ a different code. For example, if a particular series is coded such that a value of -1 indicates "not applicable", you can select "Set missing value code" under the Variable menu and type in the value "-1" (without the quotes). Gretl will then read the -1s as missing observations. 

# menu-attach Programming "Menu attachment"

This dialog enables you to specify a menu attachment for a function package. To do this you must complete the three fields in the dialog box. 

<@itl="Label"> 

This requires a short label string, which will appear as the menu entry for the package. 

<@itl="Window"> 

Select "model window" for a function package that does something with a gretl model, and should appear in the menu bar in a gretl model window. Otherwise, select "main window". 

<@itl="Menu tree"> 

Select the position within the menu tree (for either the main window or the model window, as chosen above) where the entry for the package should appear. 

# mle Estimation "Maximum likelihood estimation"

Performs Maximum Likelihood (ML) estimation using either the BFGS (Broyden, Fletcher, Goldfarb, Shanno) algorithm or Newton's method. You must specify the log-likelihood function; it is recommended that you also supply expressions for the derivatives of this function with respect to each of the parameters if possible. 

Simple example: Suppose we have a series <@lit="X"> with values 0 or 1 and we wish to obtain the maximum likelihood estimate of the probability, <@lit="p">, that <@lit="X"> = 1. (In this simple case we can guess in advance that the ML estimate of <@lit="p"> will simply equal the proportion of Xs equal to 1 in the sample.) 

The parameter <@lit="p"> must first be added to the dataset and given an initial value. This can be done using the genr command or via menu choices. Appropriate "genr" lines may be typed into the MLE specification window prior to the specification of the log-likelihood function. 

In the MLE window we type the following lines: 

<code>          
   loglik = X*log(p) + (1-X)*log(1-p)
   deriv p = X/p - (1-X)/(1-p)
</code>

The first line specifies the log-likelihood function, and the next line supplies the derivative of that function with respect to the parameter p. If no "deriv" lines are given, a numerical approximation to the derivatives is computed. 

If the parameter p was not previously declared we could preface the above lines with something like the following: 

<code>          
   scalar p = 0.5
</code>

By default, standard errors are based on the Outer Product of the Gradient. If the robust standard errors box is checked, a QML estimator is used (namely, a sandwich of the negative inverse of the Hessian and the covariance matrix of the gradient). The Hessian is approximated numerically. 

For a much more in-depth description of <@lit="mle">, please refer to <@pdf="the Gretl User's Guide">. 

Menu path: /Model/Maximum likelihood

Script command: <@ref="mle">

# modeltab Utilities "The model table"

In econometric research it is common to estimate several models with a common dependent variable—the models differing in respect of which independent variables are included, or perhaps in respect of the estimator used. In this situation it is convenient to present the regression results in the form of a table, where each column contains the results (coefficient estimates and standard errors) for a given model, and each row contains the estimates for a given variable across the models. 

Gretl provides a means of constructing such a table (and copying it in plain text, LaTeX or Rich Text Format). Here is how to do it: 

<indent>
1. Estimate a model which you wish to include in the table, and in the model display window, under the File menu, select "Save to session as icon" or "Save as icon and close". 
</indent>

<indent>
2. Repeat step 1 for the other models to be included in the table (up to a total of six models). 
</indent>

<indent>
3. When you are done estimating the models, open the icon view of your gretl session (by selecting "icon view" under the Session menu in the main gretl window, or by clicking the "session icon view" icon on the gretl toolbar). 
</indent>

<indent>
4. In session icon view, there is an icon labeled "Model table". Decide which model you wish to appear in the left-most column of the model table and add it to the table, either by dragging its icon onto the Model table icon, or by right-clicking on the model icon and selecting "Add to model table" from the pop-up menu. 
</indent>

<indent>
5. Repeat step 4 for the other models you wish to include in the table. The second model selected will appear in the second column from the left, and so on. 
</indent>

<indent>
6. When you are finished composing the model table, display it by double-clicking on its icon. Under the Edit menu in the window which appears, you have the option of copying the table to the clipboard in various formats. 
</indent>

<indent>
7. If the ordering of the models in the table is not what you wanted, right-click on the model table icon and select "Clear table". Then go back to step 4 above and try again. 
</indent>

Menu path: Session icon window, Model table icon

Script command: <@ref="modeltab">

# mpols Estimation "Multiple-precision OLS"

Computes OLS estimates for the specified model using multiple precision floating-point arithmetic, with the help of the Gnu Multiple Precision (GMP) library. By default 256 bits of precision are used for the calculations, but this can be increased via the environment variable <@lit="GRETL_MP_BITS">. For example, when using the bash shell one could issue the following command, before starting gretl, to set a precision of 1024 bits. 

<code>          
   export GRETL_MP_BITS=1024
</code>

Menu path: /Model/Other linear models/High precision OLS

Script command: <@ref="mpols">

# nadarwat Estimation "Nadaraya-Watson"

Computes the Nadaraya–Watson nonparametric estimator of the conditional mean of the dependent variable, <@itl="m(x)">, for each non-missing value of the independent variable. 

The kernel function <@itl="K"> is given by <@itl="K = exp(-x"><@sup="2"><@itl=" / 2h)"> for <@itl="|x| < T"> and zero otherwise. 

The bandwidth, usually a small number, controls the smoothness of <@itl="m(x)"> (higher values producing a smoother series); the default value is <@itl="n"><@sup="-0.2">. 

If the "leave-one-out" box is checked, a variant of the estimator is employed in which the <@itl="i">-th observation is not used in evaluating <@itl="m(x"><@sub="i"><@itl=")">. This makes the Nadaraya–Watson estimator more robust numerically and its usage is normally advised when the estimator is computed for inference purposes. 

# negbin Estimation "Negative Binomial regression"

Estimates a Negative Binomial model. The dependent variable is taken to represent a count of the occurrence of events of some sort, and must have only non-negative integer values. By default the model NegBin 2 is used, in which the conditional variance of the count is given by μ(1 + αμ), where μ denotes the conditional mean. But if the <@opt="--⁠model1"> option is given the conditional variance is μ(1 + α). 

The optional <@lit="offset"> series works in the same way as for the <@ref="poisson"> command. The Poisson model is a restricted form of the Negative Binomial in which α = 0 by construction. 

By default, standard errors are computed using a numerical approximation to the Hessian at convergence. But if the <@opt="--⁠opg"> option is given the covariance matrix is based on the Outer Product of the Gradient (OPG), or if the <@opt="--⁠robust"> option is given QML standard errors are calculated, using a "sandwich" of the inverse of the Hessian and the OPG. 

Menu path: /Model/Limited dependent variable/Count data...

Script command: <@ref="negbin">

# nls Estimation "Nonlinear Least Squares"

Performs Nonlinear Least Squares (NLS) estimation using a modified version of the Levenberg–Marquardt algorithm. You must supply a function specification; it is recommended but not required that you also supply expressions for the derivatives of this function with respect to each of the parameters if possible. If you do not supply derivatives you should instead give a list of the parameters to be estimated (separated by spaces or commas), preceded by the keyword <@lit="params">; these can be either scalars, or vectors, or any combination of the two. 

Example: Suppose we have a data set with variables <@itl="C"> and <@itl="Y"> (e.g. <@lit="greene11_3.gdt">) and we wish to estimate a nonlinear consumption function of the form 

  <@fig="greene_Cfunc">

The parameters alpha, beta and gamma must first be added to the dataset and given initial values. Appropriate lines may be typed into the NLS specification window prior to the function specification. 

In the NLS window we type the following lines: 

<code>          
   C = alpha + beta * Y^gamma
   deriv alpha = 1
   deriv beta = Y^gamma
   deriv gamma = beta * Y^gamma * log(Y)
</code>

The first line specifies the regression function, and the next three lines supply the derivatives of that function with respect to each of the parameters in turn. If the "deriv" lines are not given, a numerical approximation to the Jacobian is computed. 

If the parameters alpha, beta and gamma were not previously declared we could preface the above lines with something like the following: 

<code>          
   scalar alpha = 1
   scalar beta = 1
   scalar gamma = 1
</code>

For further details on NLS estimation please see <@pdf="the Gretl User's Guide">. 

Menu path: /Model/Nonlinear Least Squares

Script command: <@ref="nls">

# normtest Tests "Normality test"

Carries out a test for normality for the given <@var="series">. The specific test is controlled by the option flags (but if no flag is given, the Doornik–Hansen test is performed). Note: the Doornik–Hansen and Shapiro–Wilk tests are recommended over the others, on account of their superior small-sample properties. 

The test statistic and its p-value may be retrieved using the accessors <@lit="$test"> and <@lit="$pvalue">. Please note that if the <@opt="--⁠all"> option is given, the result recorded is that from the Doornik–Hansen test. 

Menu path: /Variable/Normality test

Script command: <@ref="normtest">

# nulldata Dataset "Creating a blank dataset"

Establishes a "blank" data set, containing only a constant and an index variable, with periodicity 1 and the specified number of observations. This may be used for simulation purposes: some of the <@lit="genr"> commands (e.g. <@lit="genr uniform()">, <@lit="genr normal()">) will generate dummy data from scratch to fill out the data set. This command may be useful in conjunction with <@lit="loop">. See also the "seed" option to the <@ref="set"> command. 

By default, this command cleans out all data in gretl's current workspace. If you give the <@opt="--⁠preserve"> option, however, any currently defined matrices are retained. 

Menu path: /File/New data set

Script command: <@ref="nulldata">

# ols Estimation "Ordinary Least Squares"

Computes ordinary least squares (OLS) estimates for the specified model. 

Besides coefficient estimates and standard errors, the program also prints p-values for <@itl="t"> (two-tailed) and <@itl="F">-statistics. A p-value below 0.01 indicates statistical significance at the 1 percent level and is marked with <@lit="***">. <@lit="**"> indicates significance between 1 and 5 percent and <@lit="*"> indicates significance between the 5 and 10 percent levels. Model selection statistics (the Akaike Information Criterion or AIC and Schwarz's Bayesian Information Criterion) are also printed. The formula used for the AIC is that given by <@bib="Akaike (1974);akaike74">, namely minus two times the maximized log-likelihood plus two times the number of parameters estimated. 

Menu path: /Model/Ordinary Least Squares
Other access: Beta-hat button on toolbar

Script command: <@ref="ols">

# omit Tests "Omit variables"

This command re-estimates the given model after omitting the specified variables, or after sequentially omitting insignificant variables if the relevant box is available and is checked. Besides the usual model output, it prints a test for the joint significance of the omitted variables. The null hypothesis is that the true coefficients on all the omitted variables equal zero. 

Sequential elimination works as follows: at each step the variable with the highest p-value is omitted, until all remaining variables have a p-value no greater than some cutoff. The default cutoff is 10 percent (two-sided); this can be adjusted via the spin button. By default this process operates on all variables in the model (apart from the constant). If you want to confine it to a subset of the variables, check the box labeled "Test only selected variables" and make a selection. 

Menu path: Model window, /Tests/Omit variables

Script command: <@ref="omit">

# online Dataset "Access online databases"

Gretl is able to access databases at Wake Forest University (your computer must be connected to the internet for this to work). 

Under the "File, Browse databases" menu, select the item "on database server". A window should appear, showing a listing of the gretl databases available at Wake Forest. (Depending on your location and the speed of your internet connection, this may take a few seconds.) Along with the name of the database and a short description, there will appear a "Local status" entry: this shows whether you have the database installed locally (on the hard drive of your computer) and if so, whether or not it is up to date with the version on the server. 

If you have a given database installed locally, and it is up to date, there is no advantage in accessing it via the server. But for a database that is not already installed and up to date, you may wish to get a listing of the data series: click on "Get series listing". This brings up a further window, from which you can display the values of a chosen data series, graph those values, or import them into gretl's workspace. These tasks can be accomplished using the "Series" menu, or via the popup menu that appears when you click the right mouse button on a given series. You can also search the listing for a variable of interest (the "Find" menu item). 

If you want faster access to the data, or wish to access the database offline, then select the line showing the database you want, in the initial database window, and press the "Install" button. This will download the database in compressed format, then uncompress it and install it on your hard drive. Thereafter you should be able to find it under the "File, Browse databases, gretl native" menu. 

# panel Estimation "Panel models"

Estimates a panel model. By default the fixed effects estimator is used; this is implemented by subtracting the group or unit means from the original data. 

If the "Random effects" button is checked, random effects (GLS) estimates are computed. By default the method of Swamy and Arora is used for the GLS transformation, but the Nerlove method is available as an option. 

For more details on panel estimation, please see <@pdf="the Gretl User's Guide">. 

Menu path: /Model/Panel

Script command: <@ref="panel">

# panel-between Estimation "Between groups model"

This dialog allows you to enter a specification for the "between model" in the context of panel data. This regression uses the group-means of the data, thereby ignoring the variation within the groups. This model is rarely of great interest in its own right, but may be useful for purposes of comparison (for example, with the fixed effects model). 

# panel-mode Dataset "Panel data organization"

This dialog offers up to three options with regard to defining a data set as a panel. The first two options require that the data set is already organized in a panel format (although this may not yet be recognized by gretl). The third option requires that the data set contains variables that represent the panel structure. 

<@itl="Stacked time series">: Let there be <@var="N"> cross-sectional units in the data set, and let <@var="T"> = the number of time-series observations per unit. By selecting this option you are telling gretl that the data set is currently composed of <@var="N"> consecutive blocks of <@var="T"> time-series observations, one for each cross-sectional unit. The next step will be to specify the value of <@var="N">. 

<@itl="Stacked cross sections">: You are telling gretl that the data set is currently composed of <@var="T"> consecutive blocks of <@var="N"> cross-sectional observations, one for each time period. The next step, again, will be to specify the value of <@var="N">. 

If the total number of observations in the current dataset is prime, the above options are not available. 

<@itl="Use index variables">: You are saying that the data set is currently organized any old way (it doesn't matter how), but that it contains two variables that index the cross-sectional units and the time periods respectively. The next step will be to select those two variables. Panel index variables must have nothing but non-negative integer values, with no missing values. If there are no such variables in the dataset this option is not available. 

# panel-wls Estimation "Groupwise weighted least squares"

Groupwise weighted least squares for panel data. Computes weighted least squares (WLS) estimates, with the weights based on the estimated error variances for the respective cross-sectional units in the sample. 

If the iteration option is selected, the procedure is iterated: at each round the residuals are re-computed using the current WLS parameter estimates, which gives rise to a new set of estimates of the error variances, and a hence a new set of weights. Iteration stops when the maximum difference in the parameter estimates from one round to the next falls below 0.0001 or the number of iterations reaches 20. If the iteration converges, the resulting estimates are Maximum Likelihood. 

# pca Statistics "Principal Components Analysis"

Principal Components Analysis. Prints the eigenvalues of the correlation matrix (or the covariance matrix if the option box is checked) for the variables in <@var="varlist">, along with the proportion of the joint variance accounted for by each component. Also prints the corresponding eigenvectors (or "component loadings"). 

In the window displaying the results, you have the option of saving the principal components to the dataset as series. 

Menu path: /View/Principal components
Other access: Main window pop-up (multiple selection)

Script command: <@ref="pca">

# pergm Statistics "Periodogram"

Computes and displays the spectrum of the specified series. By default the sample periodogram is given, but optionally a Bartlett lag window is used in estimating the spectrum (see, for example, Greene's <@itl="Econometric Analysis"> for a discussion of this). The default width of the Bartlett window is twice the square root of the sample size but this can be set manually using the <@var="bandwidth"> parameter, up to a maximum of half the sample size. 

If the <@opt="--⁠log"> option is given the spectrum is represented on a logarithmic scale. 

The (mutually exclusive) options <@opt="--⁠radians"> and <@opt="--⁠degrees"> influence the appearance of the frequency axis when the periodogram is graphed. By default the frequency is scaled by the number of periods in the sample, but these options cause the axis to be labeled from 0 to π radians or from 0 to 180°, respectively. 

By default, if the program is not in batch mode a plot of the periodogram is shown. This can be adjusted via the <@opt="--⁠plot"> option. The acceptable parameters to this option are <@lit="none"> (to suppress the plot); <@lit="display"> (to display a plot even when in batch mode); or a file name. The effect of providing a file name is as described for the <@opt="--⁠output"> option of the <@ref="gnuplot"> command. 

Menu path: /Variable/Periodogram
Other access: Main window pop-up menu (single selection)

Script command: <@ref="pergm">

# polyweights Transformations "Polynomial trend fitting"

In fitting a polynomial trend to a time series it may be desirable to give extra weight to the observations at the start and end of the sample. (Points in the middle of the sample range have neighbours on both sides that are likely to be pulling the fit in the same general direction.) 

The weighting schemes offered here (quadratic, cosine-bell and steps) can be used to this effect. If you select one of these schemes two additional settings must be chosen: first, what maximum weight should be used (the minimum, baseline weight is 1.0)? Second, what central fraction of the sample should be given a uniform (minimal) weighting? 

Suppose, for example, you select a maximum weight of 3.0 and a central fraction of 0.4. This means that the middle 40 percent of the data get a weight of 1.0. If the steps shape is selected the first and last 30 percent of the observations get a weight of 3.0; otherwise, for the first 30 percent of observations the weights decline gradually from 3.0 to 1.0; and for the last 30 percent the weights increase from 1.0 to 3.0. 

# poisson Estimation "Poisson estimation"

Estimates a poisson regression. The dependent variable is taken to represent the occurrence of events of some sort, and must take on only non-negative integer values. 

If a discrete random variable <@itl="Y"> follows the Poisson distribution, then 

  <@fig="poisson1">

for <@itl="y"> = 0, 1, 2,…. The mean and variance of the distribution are both equal to <@itl="v">. In the Poisson regression model, the parameter <@itl="v"> is represented as a function of one or more independent variables. The most common version (and the only one supported by gretl) has 

  <@fig="poisson2">

or in other words the log of <@itl="v"> is a linear function of the independent variables. 

Optionally, you may add an "offset" variable to the specification. This is a scale variable, the log of which is added to the linear regression function (implicitly, with a coefficient of 1.0). This makes sense if you expect the number of occurrences of the event in question to be proportional, other things equal, to some known factor. For example, the number of traffic accidents might be supposed to be proportional to traffic volume, other things equal, and in that case traffic volume could be specified as an "offset" in a Poisson model of the accident rate. The offset variable must be strictly positive. 

By default, standard errors are computed using the negative inverse of the Hessian. If the <@opt="--⁠robust"> flag is given, then QML or Huber–White standard errors are calculated instead. In this case the estimated covariance matrix is a "sandwich" of the inverse of the estimated Hessian and the outer product of the gradient. 

See also <@ref="negbin">. 

Menu path: /Model/Limited dependent variable/Count data...

Script command: <@ref="poisson">

# probit Estimation "Probit model"

If the dependent variable is a binary variable (all values are 0 or 1) maximum likelihood estimates of the coefficients on <@var="indepvars"> are obtained via the Newton–Raphson method. As the model is nonlinear the slopes depend on the values of the independent variables. By default the slopes with respect to each of the independent variables are calculated (at the means of those variables) and these slopes replace the usual p-values in the regression output. This behavior can be suppressed my giving the <@opt="--⁠p-values"> option. The chi-square statistic tests the null hypothesis that all coefficients are zero apart from the constant. 

By default, standard errors are computed using the negative inverse of the Hessian. If the "Robust standard errors" box is checked, then QML or Huber–White standard errors are calculated instead. In this case the estimated covariance matrix is a "sandwich" of the inverse of the estimated Hessian and the outer product of the gradient. See chapter 10 of Davidson and MacKinnon for details. 

If the dependent variable is not binary but is discrete, then Ordered Probit estimates are obtained. (If the variable selected as dependent is not discrete, an error is flagged.) 

With the <@opt="--⁠random-effects"> option, the error term is assumed to be composed of two normally distributed components: one time-invariant term that is specific to the cross-sectional unit or "individual" (and is known as the individual effect); and one term that is specific to the particular observation. 

Evaluation of the likelihood for this model involves the use of Gauss-Hermite quadrature for approximating the value of expectations of functions of normal variates. The number of quadrature points used can be chosen through the <@opt="--⁠quadpoints"> option (the default is 32). Using more points will increase the accuracy of the results, but at the cost of longer compute time; with many quadrature points and a large dataset estimation may be quite time consuming. 

Besides the usual parameter estimates (and associated statistics) relating to the included regressors, certain additional information is presented on estimation of this sort of model: 

<indent>
• <@lit="lnsigma2">: the maximum likelihood estimate of the log of the variance of the individual effect; 
</indent>

<indent>
• <@lit="sigma_u">: the estimated standard deviation of the individual effect; and 
</indent>

<indent>
• <@lit="rho">: the estimated share of the individual effect in the composite error variance (also known as the intra-class correlation). 
</indent>

The Likelihood Ratio test of the null hypothesis that <@lit="rho"> equals zero provides a means of assessing whether the random effects specification is needed. If the null is not rejected that suggests that a simple pooled probit specification is adequate. 

Probit for analysis of proportions is not implemented in gretl at this point. 

Menu path: /Model/Limited dependent variable/Probit

Script command: <@ref="probit">

# qlrtest Tests "Quandt likelihood ratio test"

For a model estimated on time-series data via OLS, performs the Quandt likelihood ratio (QLR) test for a structural break at an unknown point in time, with 15 percent trimming at the beginning and end of the sample period. 

For each potential break point within the central 70 percent of the observations, a Chow test is performed. See <@ref="chow"> for details; as with the regular Chow test, this is a robust Wald test if the original model was estimated with the <@opt="--⁠robust"> option, an F-test otherwise. The QLR statistic is then the maximum of the individual test statistics. 

An asymptotic p-value is obtained using the method of <@bib="Bruce Hansen (1997);hansen97">. 

Menu path: Model window, /Tests/QLR test

Script command: <@ref="qlrtest">

# qqplot Graphs "Q-Q plot"

With just one series selected, displays a plot of the empirical quantiles of the given series against the quantiles of the normal distribution. The series must include at least 20 valid observations in the current sample range. By default the empirical quantiles are plotted against quantiles of the normal distribution having the same mean and variance as the sample data, but two alternatives are available: the data may be standardized (converted to z-scores) before plotting, or the "raw" empirical quantiles may be plotted against the quantiles of the standard normal distribution. 

The option <@opt="--⁠output"> has the effect to send the output to the desiderd filename; use "display" to force output to the screen, for example during a loop. 

Given two series arguments, <@var="y"> and <@var="x">, displays a plot of the empirical quantiles of <@var="y"> against those of <@var="x">. The data values are not standardized. 

Menu path: /Variable/Normal Q-Q plot
Menu path: /View/Graph specified vars/Q-Q plot

Script command: <@ref="qqplot">

# quantreg Estimation "Quantile regression"

Quantile regression. By default standard errors are computed according to the asymptotic formula given by <@bib="Koenker and Bassett (<@itl="Econometrica">, 1978);koenker-bassett78">, but if the "robust" box is checked we use the heteroskedasticity-robust variant from <@bib="Koenker and Zhao (<@itl="Journal of Nonparametric Statistics">, 1994);koenker-zhao94">. 

If the "Compute confidence intervals" option is checked gretl will calculate confidence intervals for the coefficients, in place of standard errors. The "robust" check-box still has an effect: if it is not checked, the intervals are computed on the assumption of IID errors; with it, gretl uses the robust estimator developed by <@bib="Koenker and Machado (<@itl="Journal of the American Statistical Association">, 1999);koenker-machado99">. Note that these intervals are not just "plus or minus so many standard errors"; in general, they are asymmetrical about the point estimates of the coefficients. 

You may give a list of quantiles (see the drop-down list for some pre-defined possibilities). In that case gretl will calculate quantile estimates and either standard errors or confidence intervals for each of the specified values. 

To Follow up on the references given above, please see <@pdf="the Gretl User's Guide">. 

Menu path: /Model/Robust estimation/Quantile regression

Script command: <@ref="quantreg">

# reprobit Estimation "Random effects probit"

The random effects probit estimator provides a means of estimating a (binary) probit model for panel data. The error term is assumed to be composed of two normally distributed components: one time-invariant term that is specific to the cross-sectional unit or "individual" (and is known as the individual effect); and one term that is specific to the particular observation. 

Evaluation of the likelihood for this model involves the use of Gauss-Hermite quadrature for approximating the value of expectations of functions of normal variates. In this dialog you can select the number of quadrature points used. Using more points will increase the accuracy of the results, but at the cost of longer compute time; with many quadrature points and a large dataset estimation may be quite time consuming. 

Besides the usual parameter estimates (and associated statistics) relating to the included regressors, certain additional information is presented on estimation of this sort of model: 

<indent>
• <@lit="lnsigma2">: the maximum likelihood estimate of the log of the variance of the individual effect; 
</indent>

<indent>
• <@lit="sigma_u">: the estimated standard deviation of the individual effect; and 
</indent>

<indent>
• <@lit="rho">: the estimated share of the individual effect in the composite error variance (also known as the intra-class correlation). 
</indent>

The Likelihood Ratio test of the null hypothesis that <@lit="rho"> equals zero provides a means of assessing whether the random effects specification is needed. If the null is not rejected that suggests that a simple pooled probit specification is adequate. 

In scripting mode, the random effects probit model is estimated using the <@lit="probit"> command with the <@opt="--⁠random-effects"> option. 

# reset Tests "Ramsey's RESET"

Must follow the estimation of a model via OLS. Carries out Ramsey's RESET test for model specification (non-linearity) by adding the square and/or the cube of the fitted values to the regression and calculating the <@itl="F"> statistic for the null hypothesis that the parameters on the added terms are zero. 

Menu path: Model window, /Tests/Ramsey's RESET

Script command: <@ref="reset">

# restrict-model Tests "Restrictions on a model"

Each restriction in the set should be expressed as an equation, with a linear combination of parameters on the left and a numeric value to the right of the equals sign. Parameters may be referenced in the form <@lit="b["><@var="i"><@lit="]">, where <@var="i"> represents the position in the list of regressors (starting at 1), or <@lit="b["><@var="varname"><@lit="]">, where <@var="varname"> is the name of the regressor in question. 

The <@lit="b"> terms in the equation representing a restriction may be prefixed with a numeric multiplier, using <@lit="*"> to represent multiplication, for example <@lit="3.5*b[4]">. 

Here is an example of a set of restrictions: 

<code>          
   b[1] = 0
   b[2] - b[3] = 0
   b[4] + 2*b[5] = 1
</code>

# restrict-system Tests "Restrictions on a system of equations"

Each restriction in the set should be expressed as an equation, with a linear combination of parameters on the left and a numeric value to the right of the equals sign. Parameters are referenced using <@lit="b"> plus two numbers in square brackets. The leading number represents the position of the equation within the system and the second number indicates position in the list of regressors, starting at 1 in both cases. For example <@lit="b[2,1]"> denotes the first parameter in the second equation, and <@lit="b[3,2]"> the second parameter in the third equation. 

The <@lit="b"> terms in the equation representing a restriction may be prefixed with a numeric multiplier, using <@lit="*"> to represent multiplication, for example <@lit="3.5*b[1,4]">. 

Here is an example of a set of restrictions: 

<code>          
   b[1,1] = 0
   b[1,2] - b[2,2] = 0
   b[3,4] + 2*b[3,5] = 1
</code>

# restrict-vecm Tests "Restrictions on a VECM"

Use this command to place linear restrictions on the cointegrating relations (beta) and/or adjustment coefficients (alpha) in a vector error-correction model (VECM). 

Each restriction should be expressed as an equation, with a linear combination of parameters to the left of the equals sign and a numerical value on the right. Restrictions on beta may be non-homogeneous (non-zero on the right), but alpha restrictions must be homogeneous (zero on the right). 

If the VECM is of rank 1, the elements of beta are referenced in the form <@lit="b["><@var="i"><@lit="]">, where <@var="i"> represents position in the cointegrating vector, starting at 1. For example, <@lit="b[2]"> denotes the second element in beta. If the rank is greater than 1, use <@lit="b"> plus two numbers in square brackets. For example, <@lit="b[2,1]"> denotes the first element in the second cointegrating vector. 

To reference elements of alpha, use <@lit="a"> instead of <@lit="b">. 

The parameter identifiers in the equation representing a restriction may be prefixed with a numeric multiplier, using <@lit="*"> to represent multiplication, for example <@lit="3.5*b[4]">. 

Here is an example of a set of restrictions on a VECM of rank 1. 

<code>          
   b[1] + b[2] = 0
   b[1] + b[3] = 0
</code>

See also <@pdf="the Gretl User's Guide">. 

# rmplot Graphs "Range-mean plot"

Range–mean plot: this command creates a simple graph to help in deciding whether a time series, <@itl="y">(t), has constant variance or not. We take the full sample t=1,...,T and divide it into small subsamples of arbitrary size <@itl="k">. The first subsample is formed by <@itl="y">(1),...,<@itl="y">(k), the second is <@itl="y">(k+1), ..., <@itl="y">(2k), and so on. For each subsample we calculate the sample mean and range (= maximum minus minimum), and we construct a graph with the means on the horizontal axis and the ranges on the vertical. So each subsample is represented by a point in this plane. If the variance of the series is constant we would expect the subsample range to be independent of the subsample mean; if we see the points approximate an upward-sloping line this suggests the variance of the series is increasing in its mean; and if the points approximate a downward sloping line this suggests the variance is decreasing in the mean. 

Besides the graph, gretl displays the means and ranges for each subsample, along with the slope coefficient for an OLS regression of the range on the mean and the p-value for the null hypothesis that this slope is zero. If the slope coefficient is significant at the 10 percent significance level then the fitted line from the regression of range on mean is shown on the graph. The <@itl="t">-statistic for the null, and the corresponding p-value, are recorded and may be retrieved using the accessors <@lit="$test"> and <@lit="$pvalue"> respectively. 

Menu path: /Variable/Range-mean graph

Script command: <@ref="rmplot">

# runs Tests "Runs test"

Carries out the nonparametric "runs" test for randomness of the specified <@var="series">, where runs are defined as sequences of consecutive positive or negative values. If you want to test for randomness of deviations from the median, for a variable named <@lit="x1"> with a non-zero median, you can do the following: 

<code>          
   series signx1 = x1 - median(x1)
   runs signx1
</code>

If the <@opt="--⁠difference"> option is given, the variable is differenced prior to the analysis, hence the runs are interpreted as sequences of consecutive increases or decreases in the value of the variable. 

If the <@opt="--⁠equal"> option is given, the null hypothesis incorporates the assumption that positive and negative values are equiprobable, otherwise the test statistic is invariant with respect to the "fairness" of the process generating the sequence, and the test focuses on independence alone. 

Menu path: /Tools/Nonparametric tests

Script command: <@ref="runs">

# sampling Dataset "Setting the sample"

The Sample menu offers several ways of selecting a sub-sample from the current dataset. 

If you choose "Sample/Restrict based on criterion..." you need to supply a Boolean (logical) expression, of the same sort that you would use to define a dummy variable. For example the expression "sqft > 1400" will select only cases for which the variable sqft has a value greater than 1400. Conditions may be concatenated using the logical operators "&&" (AND) and "||" (OR), and may be negated using "!" (NOT). If the dataset already contains dummy variables, you are also given the option of selecting one of these to define the sample (observations with a value of 1 for the selected dummy will be included, and others excluded). 

The menu item "Sample/Drop all obs with missing values" redefines the sample to exclude all observations for which values of one or more variables are missing (leaving only complete cases). 

To select observations for which a particular variable has no missing values, use "Restrict based on criterion..." and supply the Boolean condition "!missing(varname)" (replace "varname" with the name of the variable you want to use). 

If the observations are labeled, you can exclude particular observations using, for example, <@lit="obs!="France""> as the Boolean criterion. The observation name must be enclosed in double quotes. 

One point should be noted about defining a sample based on a dummy variable, a Boolean expression, or on the missing values criterion: Any "structural" information in the data header file (regarding the time series or panel nature of the data) is lost. You may reimpose structure with "Sample/Set frequency, startobs...". 

Please see <@pdf="the Gretl User's Guide"> for further details. 

# save-labels Utilities "Save or remove series labels"

If you choose Export here, gretl will write a file containing the descriptive labels of any series in the current dataset that have such labels. This is a plain text file with one line per variable. The line will be empty for variables that have no descriptive label. 

If you choose Remove, the descriptive labels will be removed for all series that have such labels. This would be appropriate only if the current labels have somehow been added in error. 

# add-labels Utilities "Add series labels"

If you choose Yes here, you are offered a file-open dialog box to select a plain text file containing descriptive labels for the series in the current dataset. The file should contain one label per line; a blank line means no label. Gretl will attempt to read as many labels as there are series in the dataset, excluding the constant. 

# save-script Utilities "Save commands?"

If you choose Yes here, gretl will write a file containing a record of the commands you executed in the current session. Most commands that you execute via "point and click" have a "script" counterpart, and it is these script commands that will be saved. You could take the file as the basis for writing a gretl command script. 

If you don't care to be prompted to save a record of commands on exit, uncheck the tick box in the save commands dialog. 

# save-session Utilities "Save this gretl session?"

If you choose Yes here, gretl will write a file containing a "snapshot" of the current session, including a copy of the working dataset along with any models, graphs or other objects that you have saved "as icons". You can re-open this file later to recreate the state of gretl as of the time you quit the session (see the "File/Session files" menu). 

If you mostly work with gretl using command scripts (which we recommend for "serious" econometric work) you probably don't need to save the session, but you should be sure to save any changes to your script that you wish to keep. You may also want to save any changes to your dataset, unless these are of a sort that can easily be recreated by running a script. 

If you work with scripts and don't care to be prompted to save your session on exit, uncheck the tick box in the save session dialog. 

# scatters Graphs "Multiple pairwise graphs"

Generates pairwise graphs of the selected "Y-axis variable" against each of the selected "X-axis variables" in turn. (Or you can select several variables for the Y-axis and one for the X-axis.) Scanning a set of such plots can be a useful step in exploratory data analysis. The maximum number of plots is 16; any extra variables will be ignored. 

If the dataset is time-series, then the second sub-list can be omitted, in which case it will implicitly be taken as "time", so you can plot multiple time series in separated sub-graphs 

Menu path: /View/Multiple graphs

Script command: <@ref="scatters">

# setinfo Dataset "Edit attributes of variable"

In this dialog box you can: 

* Rename a (series) variable. 

* Add or edit a description of the variable: this appears next to the variable name in the gretl main window. 

* Add or edit the "display name" for the variable (if the variable is a series, not a scalar). This string (maximum 19 characters) is shown in place of the variable name when the variable is displayed in a graph. Thus for instance you can associate a more comprehensible string such as "T-bill rate" with a cryptically named variable such as "tb3". 

* (For time-series data) set the compaction method for the variable. This method will be used if you decide to reduce the frequency of the dataset, or if you update the variable by importing from a database where the variable is at a higher frequency than in the working dataset. 

* Mark a variable as discrete (for series with integer values only). This affects the way the variable is handled when you ask for a frequency plot. 

Menu path: /Variable/Edit attributes
Other access: Main window pop-up menu

Script command: <@ref="setinfo">

# setmiss Dataset "Missing value code"

Set a numerical value that will be interpreted as "missing" or "not applicable", either for a particular data series (under the Variable menu) or globally for the entire data set (under the Sample menu). 

Gretl has its own internal coding for missing values, but sometimes imported data may employ a different code. For example, if a particular series is coded such that a value of -1 indicates "not applicable", you can select "Set missing value code" under the Variable menu and type in the value "-1" (without the quotes). Gretl will then read the -1s as missing observations. 

Menu path: /Data/Set missing value code

Script command: <@ref="setmiss">

# spearman Statistics "Spearmans's rank correlation"

Prints Spearman's rank correlation coefficient for a specified pair of series. The series do not have to be ranked manually in advance; the function takes care of this. 

The automatic ranking is from largest to smallest (i.e. the largest data value gets rank 1). If you need to invert this ranking, create a new variable which is the negative of the original. For example: 

<code>          
   series altx = -x
   spearman altx y
</code>

Menu path: /Model/Robust estimation/Rank correlation

Script command: <@ref="spearman">

# store Dataset "Save data"

Save data to <@var="filename">. By default all currently defined series are saved but the optional <@var="varlist"> argument can be used to select a subset of series. If the dataset is sub-sampled, only the observations in the current sample range are saved. 

The format in which the data are written may be controlled in the first instance by the extension or suffix of <@var="filename">, as follows: 

<indent>
• <@lit=".gdt">, or no extension: gretl's native XML data format. (If no extension is provided, "<@lit=".gdt">" is added automatically.) 
</indent>

<indent>
• <@lit=".gtdb">: gretl's native binary data format. 
</indent>

<indent>
• <@lit=".csv">: comma-separated values (CSV). 
</indent>

<indent>
• <@lit=".txt"> or <@lit=".asc">: space-separated values. 
</indent>

<indent>
• <@lit=".R">: GNU R format. 
</indent>

<indent>
• <@lit=".m">: GNU Octave format. 
</indent>

The format-related option flags shown above can be used to force the issue of the save format independently of the filename (or to get gretl to write in the formats of PcGive or JMulTi). However, if <@var="filename"> has extension <@lit=".gdt"> or <@lit=".gdtb"> this necessarily implies use of native format and the addition of a conflicting option flag will generate an error. 

When data are saved in native format (only), the <@opt="--⁠gzipped"> option may be used for data compression, which can be useful for large datasets. The optional parameter for this flag controls the level of compression (from 0 to 9): higher levels produce a smaller file, but compression takes longer. The default level is 1; a level of 0 means that no compression is applied. 

The option flags <@opt="--⁠omit-obs"> and <@opt="--⁠no-header"> are applicable only when saving data in CSV format. By default, if the data are time series or panel, or if the dataset includes specific observation markers, the CSV file includes a first column identifying the observations (e.g. by date). If the <@opt="--⁠omit-obs"> flag is given this column is omitted. The <@opt="--⁠no-header"> flag suppresses the usual printing of the names of the variables at the top of the columns. 

The option flag <@opt="--⁠decimal-comma"> is also confined to the case of saving data in CSV format. The effect of this option is to replace the decimal point with the decimal comma; in addition the column separator is forced to be a semicolon. 

The option of saving in gretl database format is intended to help with the construction of large sets of series, possibly having mixed frequencies and ranges of observations. At present this option is available only for annual, quarterly or monthly time-series data. If you save to a file that already exists, the default action is to append the newly saved series to the existing content of the database. In this context it is an error if one or more of the variables to be saved has the same name as a variable that is already present in the database. The <@opt="--⁠overwrite"> flag has the effect that, if there are variable names in common, the newly saved variable replaces the variable of the same name in the original dataset. 

The <@opt="--⁠comment"> option is available when saving data as a database or in CSV format. The required parameter is a double-quoted one-line string, attached to the option flag with an equals sign. The string is inserted as a comment into the database index file or at the top of the CSV output. 

The <@lit="store"> command behaves in a special manner in the context of a "progressive loop". See <@pdf="the Gretl User's Guide"> for details. 

Menu path: /File/Save data; /File/Export data

Script command: <@ref="store">

# system Estimation "Systems of equations"

In this window you can define a system of equations and choose an estimator for the system. Four sorts of statement may be given here, as follows: 

<indent>
• <@ref="equation">: specify an equation within the system. At least two such statements must be provided. 
</indent>

<indent>
• <@lit="instr">: for a system to be estimated via Three-Stage Least Squares, a list of instruments (by variable name or number). Alternatively, you can put this information into the <@lit="equation"> line using the same syntax as in the <@ref="tsls"> command. 
</indent>

<indent>
• <@lit="endog">: for a system of simultaneous equations, a list of endogenous variables. This is primarily intended for use with FIML estimation, but with Three-Stage Least Squares this approach may be used instead of giving an <@lit="instr"> list; then all the variables not identified as endogenous will be used as instruments. 
</indent>

<indent>
• <@lit="identity">: for use with FIML, an identity linking two or more of the variables in the system. This sort of statement is ignored when an estimator other than FIML is used. 
</indent>

Menu path: /Model/Simultaneous equations

Script command: <@ref="system">

# tobit Estimation "Tobit model"

Estimates a Tobit model, which may be appropriate when the dependent variable is "censored". For example, positive and zero values of purchases of durable goods on the part of individual households are observed, and no negative values, yet decisions on such purchases may be thought of as outcomes of an underlying, unobserved disposition to purchase that may be negative in some cases. 

By default it is assumed that the dependent variable is censored at zero on the left and is uncensored on the right. However you can use the entry boxes marked "left bound" and "right bound" to specify a different pattern of censoring. Enter either a numerical value or <@lit="NA"> for no censoring. 

The Tobit model is a special case of interval regression, which is supported via the <@ref="intreg"> command. 

Menu path: /Model/Limited dependent variable/Tobit

Script command: <@ref="tobit">

# transpos Dataset "Transpose data"

Transposes the current data set. That is, each observation (row) in the current data set will be treated as a variable (column), and each variable as an observation. This command may be useful if data have been read from some external source in which the rows of the data table represent variables. 

See also <@ref="dataset">. 

Menu path: /Data/Transpose data

# tsls Estimation "Instrumental variables regression"

This command requires the selection of two lists of variables: the independent variables to appear in the given model and a set of instruments. Note that any exogenous regressors should appear in both lists. 

Output for two-stage least squares estimates includes the Hausman test and, if the model is over-identified, the Sargan over-identification test. In the Hausman test, the null hypothesis is that OLS estimates are consistent, or in other words estimation by means of instrumental variables is not really required. A model of this sort is over-identified if there are more instruments than are strictly required. The Sargan test is based on an auxiliary regression of the residuals from the two-stage least squares model on the full list of instruments. The null hypothesis is that all the instruments are valid, and suspicion is thrown on this hypothesis if the auxiliary regression has a significant degree of explanatory power. For a good explanation of both tests see chapter 8 of <@bib="Davidson and MacKinnon (2004);davidson-mackinnon04">. 

For both TSLS and LIML estimation, an additional test result is shown provided that the model is estimated under the assumption of i.i.d. errors (that is, the <@opt="--⁠robust"> option is not selected). This is a test for weakness of the instruments. Weak instruments can lead to serious problems in IV regression: biased estimates and/or incorrect size of hypothesis tests based on the covariance matrix, with rejection rates well in excess of the nominal significance level <@bib="(Stock, Wright and Yogo, 2002);stock-wright-yogo02">. The test statistic is the first-stage <@itl="F">-test if the model contains just one endogenous regressor, otherwise it is the smallest eigenvalue of the matrix counterpart of the first stage <@itl="F">. Critical values based on the Monte Carlo analysis of <@bib="Stock and Yogo (2003);stock-yogo03"> are shown when available. 

The R-squared value printed for models estimated via two-stage least squares is the square of the correlation between the dependent variable and the fitted values. 

Menu path: /Model/Instrumental variables

Script command: <@ref="tsls">

# var Estimation "Vector Autoregression"

This command requires specification of: 

<indent>
• - the lag order, that is, the number of lags of each variable that should be included in the system; 
</indent>

<indent>
• - any exogenous variables (but note that a constant is included automatically unless you specify otherwise, a trend can be added using the trend checkbox, and seasonal dummy variables can be added using the seasonals checkbox); and 
</indent>

<indent>
• - a list of endogenous variables, lags of which will be included on the right-hand side of each equation (note: do not include lagged variables in this list -- they will be added automatically). 
</indent>

A separate regression will be run for each variable in the system. Output for each equation includes F-tests for zero restrictions on all lags of each of the variables and an F-test for the maximum lag, along with (optionally) forecast variance decompositions and impulse response functions. 

Forecast variance decompositions and impulse responses are based on the Cholesky decomposition of the contemporaneous covariance matrix, and in this context the order in which the (stochastic) variables are given matters. The first variable in the list is assumed to be "most exogenous" within-period. The horizon for variance decompositions and impulse responses can be set using the <@ref="set"> command. For retrieval of a specified impulse response function in matrix form, see the <@xrf="irf"> function. 

Menu path: /Model/Time series/Vector autoregression

Script command: <@ref="var">

# VAR-lagselect Tests "VAR lag-length selection"

In this dialog box you specify a VAR as usual, but use the lag order spin button to set the maximum number of lags to test. 

Output will consist of a table showing the values of the Akaike (AIC), Schwartz (BIC) and Hannan–Quinn (HQC) information criteria computed from VARs of order 1 to the chosen maximum. This is intended to help with the selection of the optimal lag order. 

# VAR-omit Tests "Test exogenous variables in VAR"

Use this dialog box to specify a subset of exogenous variables in a VAR. These variables will be omitted from the original VAR, and the system re-estimated. 

A Likelihood Ratio test is reported, where the null hypothesis is that the true parameter values are zero, in all equations of the VAR, for the omitted variables. The test is based on the difference between the log-determinant of the variance matrix for the unrestricted system, and that for the restricted system with the selected variables omitted. 

# vartest Tests "Difference of variances"

Calculates the <@itl="F"> statistic for the null hypothesis that the population variances are equal for the two selected series, and shows its p-value. 

Menu path: /Tools/Test statistic calculator

Script command: <@ref="vartest">

# vecm Estimation "Vector Error Correction Model"

A VECM is a form of vector autoregression or VAR (see <@ref="var">), applicable where the variables in the model are individually integrated of order 1 (that is, are random walks, with or without drift), but exhibit cointegration. This command is closely related to the Johansen test for cointegration (see <@ref="coint2">). 

The lag order selected in the VECM dialog box is that of the VAR system. The number of lags in the VECM itself (where the dependent variable is given as a first difference) is one less than this number. 

The "rank" represents the number of cointegrating vectors. This must be greater than zero and less than or equal to (generally, less than) the number of endogenous variables selected. 

In the "Endogenous variables" box you select the vector of endogenous variables, in levels. The inclusion of deterministic terms in the model is controlled by the option buttons. The default is to include an "unrestricted constant", which allows for the presence of a non-zero intercept in the cointegrating relations as well as a trend in the levels of the endogenous variables. In the literature stemming from the work of Johansen (see for example his 1995 book) this is often referred to as "case 3". The other four options produce cases 1, 2, 4 and 5 respectively. The meaning of these cases and the criteria for selecting a case are explained in <@pdf="the Gretl User's Guide">. 

In the "Exogenous variables" box you may add specific exogenous variables. By default these enter the model in unrestricted form (indicated by a <@lit="U"> next to the name of the variable). If you want a certain exogenous variable to be restricted to the cointegrating space, right-click on it and select "Restricted" from the pop-up menu. The symbol next to the variable will change to R. 

If the data are quarterly or monthly, a check box is shown that allows you to include a set of centered seasonal dummy variables. In all cases, an additional check box ("Show details") allows for the printing of the auxiliary regressions that form the starting point of the Johansen maximum likelihood estimation procedure. 

Menu path: /Model/Time series/VECM

Script command: <@ref="vecm">

# wls Estimation "Weighted Least Squares"

Let "wtvar" denote the variable selected in the "Weight variable" box. An OLS regression is run, where the dependent variable is the product of the positive square root of wtvar and the selected dependent variable, and the independent variables are also multiplied by the square root of wtvar. Statistics such as <@itl="R">-squared are based on the weighted data. If wtvar is a dummy variable, weighted least squares estimation is equivalent to eliminating all observations with value zero for wtvar. 

Menu path: /Model/Other linear models/Weighted Least Squares

Script command: <@ref="wls">

# working-dir Utilities "Working directory"

The "working directory" is where gretl looks by default when reading or writing data files or scripts via the file Open and Save dialogs. 

In addition the working directory is the default location for 

<indent>
• reading files via the script commands <@lit="append">, <@lit="open">, <@lit="run"> and <@lit="include">; and 
</indent>

<indent>
• writing files via the commands <@lit="eqnprint">, <@lit="tabprint">, <@lit="gnuplot">, <@lit="outfile"> and <@lit="store">. 
</indent>

The option of having gretl use the current directory (as determined via the shell) at start-up may be useful to people who are in the habit of launching gretl from a command prompt rather than a menu or icon. 

This dialog also allows you to set the behavior of the GUI file selector: when you open or save a file in a given folder, should the selector remember and return to the same folder on the next invocation? Or should the selector always visit the chosen working directory? 

Menu path: /File/Working directory

# x12a Utilities "X-12-ARIMA"

There are two procedural options here, controlled by the lower set of radio-buttons. 

If you select "Execute X-12-ARIMA directly" then gretl writes a command file for X-12-ARIMA and calls the x12a program to execute the commands. In this case you have the option of producing a graph and/or saving selected output series to the gretl dataset. 

If you select "Make X-12-ARIMA command file" gretl writes a command file for X-12-ARIMA, as above, but then opens this file in an editor window. In that window you are able to make changes and to save the file under a chosen name. You are also able to send the file for execution by x12a (by clicking the "Run" button on the editor window toolbar) and view the output. But in this case you do not have the option of saving data as gretl series or producing a gretl graph. 

# xcorrgm Statistics "Cross-correlogram"

Prints and graphs the cross-correlogram for <@var="series1"> and <@var="series2">, which may be specified by name or number. The values are the sample correlation coefficients between the current value of <@var="series1"> and successive leads and lags of <@var="series2">. 

If an <@var="order"> value is specified the length of the cross-correlogram is limited to at most that number of leads and lags, otherwise the length is determined automatically, as a function of the frequency of the data and the number of observations. 

By default, a plot of the cross-correlogram is produced: a gnuplot graph in interactive mode or an ASCII graphic in batch mode. This can be adjusted via the <@opt="--⁠plot"> option. The acceptable parameters to this option are <@lit="none"> (to suppress the plot); <@lit="ascii"> (to produce a text graphic even when in interactive mode); <@lit="display"> (to produce a gnuplot graph even when in batch mode); or a file name. The effect of providing a file name is as described for the <@opt="--⁠output"> option of the <@ref="gnuplot"> command. 

Menu path: /View/Cross-correlogram
Other access: Main window pop-up menu (multiple selection)

Script command: <@ref="xcorrgm">

# xtab Statistics "Cross-tabulate variables"

Displays a contingency table or cross-tabulation for each combination of the selected variables. Note that all the variables must be discrete. 

By default, frequency count values are shown in the cells and on the margins of the table. However, you can choose to display either row or column percentages instead. 

By default, cells with a zero count are shown as empty, but you can choose to show zero values explicitly. 

Pearson's chi-square test for independence is displayed if the expected frequency under independence is at least 1.0e-7 for all cells. A common rule of thumb for the validity of this statistic is that at least 80 percent of cells should have expected frequencies of 5 or greater; if this criterion is not met a warning is printed. 

If the contingency table is 2 by 2, Fisher's Exact Test for independence is computed. Note that this test is based on the assumption that the row and column totals are fixed, which may or may not be approriate depending on how the data were generated. The left p-value should be used when the alternative to independence is negative association (values tend to cluster in the lower left and upper right cells); the right p-value should be used if the alternative is positive association. The two-tailed p-value for this test is calculated by method (b) in section 2.1 of <@bib="Agresti (1992);agresti92">: it is the sum of the probabilities of all possible tables having the given row and column totals and having a probability less than or equal to that of the observed table. 

Script command: <@ref="xtab">
